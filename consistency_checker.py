"""æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥ä¸ä¿®æ”¹æ¨¡å— - åŸºäºå¤–éƒ¨RAGç³»ç»Ÿ"""
import json
import asyncio
from typing import List, Dict, Set
from pathlib import Path
from openai import AsyncOpenAI
import config
from rag_tool import RAGTool
from knowledge_base import KnowledgeBaseManager
from logger import get_logger

logger = get_logger(__name__)


class ConsistencyChecker:
    """æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥å™¨ - åˆ©ç”¨å¤–éƒ¨RAGç³»ç»Ÿ"""
    
    def __init__(self, api_key: str = None, base_url: str = None, model: str = None, project_id: str = None):
        self.model = model or config.MODEL_NAME
        self.client = AsyncOpenAI(
            api_key=api_key or config.OPENROUTER_API_KEY,
            base_url=base_url or config.OPENROUTER_BASE_URL
        )
        self.rag_tool = RAGTool()
        self.kb_manager = KnowledgeBaseManager()
        self.project_id = project_id  # ä¿å­˜é¡¹ç›®IDï¼Œç”¨äºReactAgent
    
    async def find_related_documents(self, 
                                     modification_point: str,
                                     project_id: str,
                                     current_file: str = None,
                                     top_k: int = 10) -> Dict:
        """
        æŸ¥æ‰¾ä¸ä¿®æ”¹ç‚¹ç›¸å…³çš„æ‰€æœ‰æ–‡æ¡£
        
        Args:
            modification_point: ä¿®æ”¹çš„å†…å®¹ç‚¹ï¼ˆå¦‚"æ—©å­£åˆ†ç±»"ï¼‰
            project_id: é¡¹ç›®ID
            current_file: å½“å‰æ­£åœ¨ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œç”¨äºæ’é™¤ï¼‰
            top_k: RAGå¬å›æ•°é‡
            
        Returns:
            {
                "related_files": {
                    "file_path1": [chunk1, chunk2, ...],
                    "file_path2": [chunk3, ...]
                },
                "total_files": int,
                "total_chunks": int
            }
        """
        logger.info(f"æŸ¥æ‰¾ä¸ '{modification_point}' ç›¸å…³çš„æ–‡æ¡£...")
        
        # è°ƒç”¨RAGæ£€ç´¢ï¼Œä½¿ç”¨metadata_filterç­›é€‰file_chunk
        search_result = await self.rag_tool.search(
            query=modification_point,
            project_id=project_id,
            top_k=top_k,
            use_refine=False,
            metadata_filter={"content_type": "file_chunk"}
        )
        
        if not search_result["success"] or not search_result["data"]:
            logger.warning("RAGæ£€ç´¢æœªè¿”å›ç»“æœ")
            return {
                "related_files": {},
                "total_files": 0,
                "total_chunks": 0
            }
        
        data = search_result["data"]
        
        # RAGè¿”å›çš„æ•°æ®å¯èƒ½æœ‰å¤šç§ç»“æ„ï¼Œéœ€è¦çµæ´»å¤„ç†
        all_chunks = []
        
        # æ–¹å¼1: ç›´æ¥çš„bundlesæ•°ç»„ï¼ˆæ¯ä¸ªbundleåŒ…å«conversations/factsï¼‰
        if data.get("bundles"):
            for bundle in data["bundles"]:
                # ä»conversationsä¸­æå–
                for conv in bundle.get("conversations", []):
                    all_chunks.append({
                        "content": conv.get("text", ""),
                        "score": conv.get("score", 1.0),
                        "metadata": conv.get("metadata", {})
                    })
                
                # ä»factsä¸­æå–ï¼ˆä¹Ÿå¯èƒ½åŒ…å«ç›¸å…³ä¿¡æ¯ï¼‰
                for fact in bundle.get("facts", []):
                    all_chunks.append({
                        "content": fact.get("content", ""),
                        "score": fact.get("score", 1.0),
                        "metadata": fact.get("metadata", {})
                    })
            
            if all_chunks:
                logger.info(f"ä»bundlesä¸­æå–åˆ° {len(all_chunks)} ä¸ªchunks")
        
        # æ–¹å¼2: short_term_memoryæ ¼å¼ï¼ˆæ—§ç‰ˆRAGï¼‰
        elif data.get("short_term_memory"):
            short_term_memory = data["short_term_memory"]
            
            # ä»conversationsæå–
            for conv in short_term_memory.get("conversations", []):
                all_chunks.append({
                    "content": conv.get("text", ""),
                    "score": 1.0,
                    "metadata": conv.get("metadata", {})
                })
            
            # ä»factsæå–
            for fact in short_term_memory.get("facts", []):
                all_chunks.append({
                    "content": fact.get("content", ""),
                    "score": 1.0,
                    "metadata": fact.get("metadata", {})
                })
            
            if all_chunks:
                logger.info(f"ä»short_term_memoryä¸­æå–åˆ° {len(all_chunks)} ä¸ªchunks")
        
        # ä½¿ç”¨æå–åˆ°çš„chunks
        bundles = all_chunks
        
        # æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„chunks
        related_files = {}
        for i, bundle in enumerate(bundles):
            # ä»bundleä¸­æå–æ–‡ä»¶è·¯å¾„å’Œå†…å®¹
            metadata = bundle.get("metadata", {})
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„å­—æ®µåï¼ˆä¸åŒRAGç‰ˆæœ¬å¯èƒ½ä½¿ç”¨ä¸åŒå­—æ®µï¼‰
            file_identifier = (
                metadata.get("file_path") or          # ä¼˜å…ˆä½¿ç”¨file_path
                metadata.get("source_identifier") or  # å…¶æ¬¡source_identifier
                metadata.get("minio_url") or          # ç„¶åminio_url
                metadata.get("source") or             # æœ€åsource
                "unknown"
            )
            
            # è°ƒè¯•ï¼šè¾“å‡ºå‰2ä¸ªchunkçš„metadata
            if i < 2:
                logger.info(f"Chunk {i} - å¯ç”¨å­—æ®µ: {list(metadata.keys())}")
                logger.info(f"Chunk {i} - æå–çš„file_identifier: {file_identifier}")
            
            # è·³è¿‡æ— æ•ˆURL
            if file_identifier == "unknown" or not file_identifier.startswith("http"):
                logger.warning(f"è·³è¿‡æ— æ•ˆçš„file_identifier: {file_identifier} (metadata keys: {list(metadata.keys())})")
                continue
            
            # è·³è¿‡å½“å‰æ­£åœ¨ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            if current_file and file_identifier == current_file:
                continue
            
            if file_identifier not in related_files:
                related_files[file_identifier] = []
            
            related_files[file_identifier].append({
                "content": bundle.get("content", ""),
                "score": bundle.get("score", 0),
                "metadata": metadata
            })
        
        if related_files:
            logger.info(f"æ–‡ä»¶æ ‡è¯†ç¬¦ç¤ºä¾‹: {list(related_files.keys())[:2]}")
        
        logger.info(f"æ‰¾åˆ° {len(related_files)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œå…± {len(bundles)} ä¸ªchunks")
        
        return {
            "related_files": related_files,
            "total_files": len(related_files),
            "total_chunks": len(bundles)
        }
    
    async def analyze_consistency(self,
                                  modification_request: str,
                                  current_file_content: str,
                                  related_files_content: Dict[str, str]) -> Dict:
        """
        åˆ†ææ–‡æ¡£é—´çš„ä¸€è‡´æ€§ï¼Œåˆ¤æ–­å“ªäº›æ–‡æ¡£éœ€è¦åŒæ­¥ä¿®æ”¹
        
        Args:
            modification_request: ç”¨æˆ·çš„ä¿®æ”¹è¦æ±‚
            current_file_content: å½“å‰æ–‡ä»¶å†…å®¹ï¼ˆå¯é€‰ï¼Œå¯èƒ½ä¸ºNoneï¼‰
            related_files_content: {file_path: file_content}
            
        Returns:
            {
                "needs_modification": [file_path1, file_path2, ...],
                "modification_type": str,
                "consistency_analysis": str,
                "global_consistency_required": bool
            }
        """
        # å¦‚æœæ²¡æœ‰å…¶ä»–æ–‡ä»¶ï¼Œç›´æ¥è¿”å›éœ€è¦ä¿®æ”¹
        if not related_files_content:
            return {
                "needs_modification": [],
                "modification_type": "æ–‡æ¡£ä¿®æ”¹",
                "consistency_analysis": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
                "global_consistency_required": False
            }
        
        # æ„å»ºåˆ†æprompt
        files_summary = []
        for file_path, content in list(related_files_content.items())[:5]:  # æœ€å¤šåˆ†æ5ä¸ªæ–‡ä»¶
            # ä»è·¯å¾„æå–æ–‡ä»¶å
            file_name = file_path.split('/')[-1] if '/' in file_path else file_path.split('\\')[-1]
            files_summary.append(
                f"æ–‡ä»¶: {file_name}\n"
                f"å†…å®¹é¢„è§ˆ: {content[:300]}...\n"
            )
        
        # å¦‚æœæä¾›äº†å½“å‰æ–‡ä»¶å†…å®¹ï¼ŒåŒ…å«åœ¨åˆ†æä¸­
        current_file_section = ""
        if current_file_content:
            current_file_section = f"""
å½“å‰æ–‡ä»¶å†…å®¹é¢„è§ˆ:
{current_file_content[:500]}...
"""
        
        analysis_prompt = f"""ä½ éœ€è¦åˆ†æä»¥ä¸‹ä¿®æ”¹éœ€æ±‚å¯¹æ–‡æ¡£çš„å½±å“ã€‚

ä¿®æ”¹è¦æ±‚:
{modification_request}
{current_file_section}
ç›¸å…³æ–‡æ¡£:
{chr(10).join(files_summary)}

è¯·åˆ†æ:
1. æ ¹æ®ä¿®æ”¹è¦æ±‚ï¼Œå“ªäº›æ–‡æ¡£éœ€è¦ä¿®æ”¹ï¼Ÿ
2. ä¿®æ”¹ç±»å‹æ˜¯ä»€ä¹ˆï¼ˆæœ¯è¯­ç»Ÿä¸€/æ•°æ®æ›´æ–°/æ–¹æ³•æ”¹è¿›ç­‰ï¼‰ï¼Ÿ
3. ä¸ºä»€ä¹ˆè¿™äº›æ–‡æ¡£éœ€è¦ä¿®æ”¹ï¼Ÿ

ä»¥JSONæ ¼å¼è¿”å›:
{{
  "needs_modification": ["file1.md", "file2.md"],  // éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨
  "modification_type": "æœ¯è¯­ç»Ÿä¸€/è§‚ç‚¹è°ƒæ•´/æ•°æ®æ›´æ–°ç­‰",
  "consistency_analysis": "è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›æ–‡æ¡£éœ€è¦ä¿®æ”¹",
  "global_consistency_required": true/false
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            logger.info("åˆ†ææ–‡æ¡£ä¸€è‡´æ€§...")
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ä¸€è‡´æ€§åˆ†æå¸ˆã€‚"},
                    {"role": "user", "content": analysis_prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            
            content = response.choices[0].message.content.strip()
            
            # æå–JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            analysis = json.loads(content)
            logger.info(f"ä¸€è‡´æ€§åˆ†æå®Œæˆ: {json.dumps(analysis, ensure_ascii=False)}")
            return analysis
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
            # é»˜è®¤æ‰€æœ‰ç›¸å…³æ–‡ä»¶éƒ½éœ€è¦ä¿®æ”¹
            return {
                "needs_modification": list(related_files_content.keys()),
                "modification_type": "ä¸€è‡´æ€§ä¿®æ”¹",
                "consistency_analysis": "é»˜è®¤å…¨éƒ¨åŒæ­¥ä¿®æ”¹",
                "global_consistency_required": True
            }
    
    async def generate_modifications(self,
                                    modification_request: str,
                                    current_modification: str,
                                    files_to_modify: Dict[str, str],
                                    project_id: str = None) -> List[Dict]:
        """
        ä¸ºéœ€è¦ä¿®æ”¹çš„æ–‡æ¡£ç”Ÿæˆä¿®æ”¹ç‰ˆæœ¬
        
        Args:
            modification_request: ä¿®æ”¹è¦æ±‚
            current_modification: å½“å‰æ–‡ä»¶çš„ä¿®æ”¹ç¤ºä¾‹
            files_to_modify: {file_path: file_content}
            
        Returns:
            [
                {
                    "file_path": str,
                    "original_content": str,
                    "modified_content": str,
                    "diff_summary": str
                }
            ]
        """
        logger.info(f"ğŸš€ å¹¶è¡Œå¤„ç† {len(files_to_modify)} ä¸ªæ–‡æ¡£çš„ä¿®æ”¹...")
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡æ¡£
        tasks = [
            self._modify_single_file(
                modification_request,
                current_modification,
                file_path,
                original_content,
                project_id=project_id or self.project_id
            )
            for file_path, original_content in files_to_modify.items()
        ]
        
        modifications = await asyncio.gather(*tasks)
        
        logger.info(f"âœ… {len(modifications)} ä¸ªæ–‡æ¡£å¤„ç†å®Œæˆ")
        return list(modifications)
    
    async def _modify_single_file(self,
                                  modification_request: str,
                                  current_modification: str,
                                  minio_url: str,
                                  original_content: str,
                                  project_id: str = None) -> Dict:
        """
        ä¿®æ”¹å•ä¸ªæ–‡ä»¶ - æ–°æµç¨‹ï¼šAIè¯„ä¼° â†’ ReactAgentç”Ÿæˆ â†’ Diff
        
        æµç¨‹ï¼š
        1. AIè¯„ä¼°éœ€è¦ä¿®æ”¹çš„ç‚¹ï¼ˆä¸ç”Ÿæˆå…·ä½“ä¿®æ”¹å†…å®¹ï¼‰
        2. å°†è¯„ä¼°ç»“æœå’ŒåŸæ–‡ä¼ ç»™ReactAgentç”Ÿæˆä¿®æ”¹åçš„å†…å®¹
        3. ç”Ÿæˆdiff
        """
        # ä»URLæå–æ–‡ä»¶å
        file_name = minio_url.split('/')[-1] if '/' in minio_url else minio_url
        
        try:
            # ========== ç¬¬1æ­¥ï¼šAIè¯„ä¼°éœ€è¦ä¿®æ”¹çš„ç‚¹ ==========
            logger.info(f"ğŸ” ç¬¬1æ­¥ï¼šAIè¯„ä¼°æ–‡æ¡£ä¿®æ”¹ç‚¹: {file_name}")
            evaluation = await self._evaluate_modification_points(
                modification_request,
                current_modification,
                file_name,
                original_content
            )
            
            if not evaluation.get("needs_modification", True):
                logger.info(f"â„¹ï¸ AIè®¤ä¸ºæ–‡æ¡£ {file_name} æ— éœ€ä¿®æ”¹")
                return {
                    "file_path": minio_url,
                    "original_content": original_content,
                    "modified_content": original_content,
                    "diff_summary": "æ— éœ€ä¿®æ”¹",
                    "original_length": len(original_content),
                    "modified_length": len(original_content),
                    "evaluation": evaluation,
                    "react_thinking_process": [],
                    "react_search_history": [],
                    "truncated": False
                }
            
            # ========== ç¬¬2æ­¥ï¼šè°ƒç”¨ReactAgentç”Ÿæˆä¿®æ”¹åçš„å†…å®¹ ==========
            logger.info(f"ğŸ¤– ç¬¬2æ­¥ï¼šè°ƒç”¨ReactAgentç”Ÿæˆä¿®æ”¹åçš„å†…å®¹")
            react_result = await self._generate_with_react_agent(
                modification_request,
                original_content,
                evaluation,
                project_id=project_id or self.project_id
            )
            
            modified_content = react_result.get("content", original_content)
            thinking_process = react_result.get("thinking_process", [])
            search_history = react_result.get("search_history", [])
            
            # ========== ç¬¬3æ­¥ï¼šç”ŸæˆDiff ==========
            logger.info(f"ğŸ“Š ç¬¬3æ­¥ï¼šç”Ÿæˆdiff")
            diff_summary = f"âœ… ReactAgentå·²ç”Ÿæˆä¿®æ”¹å†…å®¹"
            
            return {
                "file_path": minio_url,
                "original_content": original_content,
                "modified_content": modified_content,
                "diff_summary": diff_summary,
                "original_length": len(original_content),
                "modified_length": len(modified_content),
                "evaluation": evaluation,
                "react_thinking_process": thinking_process,  # ReactAgentæ€è€ƒè¿‡ç¨‹
                "react_search_history": search_history,  # ReactAgentæœç´¢å†å²
                "truncated": False
            }
            
        except Exception as e:
            logger.error(f"ä¿®æ”¹æ–‡ä»¶å¤±è´¥ {minio_url}: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "file_path": minio_url,
                "original_content": original_content,
                "modified_content": original_content,
                "diff_summary": f"ä¿®æ”¹å¤±è´¥: {str(e)}",
                "original_length": len(original_content),
                "modified_length": len(original_content),
                "evaluation": {},
                "react_thinking_process": [],
                "react_search_history": [],
                "truncated": False
            }
    
    async def _evaluate_modification_points(self,
                                           modification_request: str,
                                           current_modification: str,
                                           file_name: str,
                                           original_content: str) -> Dict:
        """
        AIè¯„ä¼°é˜¶æ®µï¼šåªè¯„ä¼°éœ€è¦ä¿®æ”¹çš„ç‚¹ï¼Œä¸ç”Ÿæˆå…·ä½“ä¿®æ”¹å†…å®¹
        
        Returns:
            {
                "needs_modification": True/False,
                "modification_points": [
                    {
                        "location": "ç« èŠ‚åç§°æˆ–ä½ç½®",
                        "original_text": "éœ€è¦ä¿®æ”¹çš„åŸå§‹æ–‡æœ¬ç‰‡æ®µ",
                        "modification_reason": "ä¸ºä»€ä¹ˆéœ€è¦ä¿®æ”¹",
                        "modification_type": "ä¿®æ”¹ç±»å‹ï¼ˆå¦‚æœ¯è¯­ç»Ÿä¸€ã€å†…å®¹è¡¥å……ç­‰ï¼‰"
                    }
                ],
                "overall_guidance": "æ•´ä½“ä¿®æ”¹æŒ‡å¯¼"
            }
        """
        # æ„å»ºè¯„ä¼°prompt
        reference_section = ""
        if current_modification:
            reference_section = f"""
å‚è€ƒä¿®æ”¹ç¤ºä¾‹ï¼ˆä¿æŒä¸€è‡´çš„ä¿®æ”¹é£æ ¼ï¼‰:
{current_modification[:500]}...
"""
        
        evaluation_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£è¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼Œè¯„ä¼°éœ€è¦ä¿®æ”¹çš„ç‚¹ã€‚

ä¿®æ”¹è¦æ±‚:
{modification_request}
{reference_section}
å¾…è¯„ä¼°æ–‡ä»¶: {file_name}
æ–‡ä»¶å†…å®¹:
{original_content}

ä½ çš„ä»»åŠ¡æ˜¯**è¯„ä¼°å¹¶æå–**éœ€è¦ä¿®æ”¹çš„ä½ç½®ï¼š
1. **è¯†åˆ«ä¿®æ”¹ç‚¹**: æ‰¾å‡ºæ–‡æ¡£ä¸­å“ªäº›ç« èŠ‚/æ®µè½éœ€è¦ä¿®æ”¹
2. **ç²¾ç¡®æå–åŸæ–‡**: ä»æ–‡æ¡£ä¸­é€å­—å¤åˆ¶éœ€è¦ä¿®æ”¹çš„åŸæ–‡ç‰‡æ®µï¼ˆç”¨äºå®šä½ï¼‰
3. **è¯´æ˜åŸå› **: è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦ä¿®æ”¹è¿™äº›éƒ¨åˆ†
4. **åˆ†ç±»ä¿®æ”¹**: è¯´æ˜ä¿®æ”¹ç±»å‹ï¼ˆå¦‚æœ¯è¯­ç»Ÿä¸€ã€å†…å®¹è¡¥å……ã€è§‚ç‚¹è°ƒæ•´ç­‰ï¼‰

**è¾“å‡ºæ ¼å¼**: ä½¿ç”¨ä»¥ä¸‹JSONæ ¼å¼ï¼š
```json
{{
  "needs_modification": true/false,
  "modification_points": [
    {{
      "location": "æ¸…æ™°çš„ä½ç½®æè¿°ï¼ˆå¦‚'ç¬¬1ç«  Introduction ç¬¬ä¸€æ®µ'ï¼‰",
      "original_text": "ä»æ–‡æ¡£ä¸­é€å­—ç²¾ç¡®å¤åˆ¶çš„åŸæ–‡ç‰‡æ®µï¼ˆå®Œæ•´çš„æ®µè½æˆ–å¥å­ï¼Œç”¨äºç²¾ç¡®å®šä½ï¼‰",
      "modification_reason": "ä¸ºä»€ä¹ˆéœ€è¦ä¿®æ”¹è¿™éƒ¨åˆ†",
      "modification_type": "ä¿®æ”¹ç±»å‹"
    }}
  ],
  "overall_guidance": "æ•´ä½“ä¿®æ”¹æŒ‡å¯¼è¯´æ˜"
}}
```

**å…³é”®è¦æ±‚**:
- **original_textå¿…é¡»ä»æ–‡æ¡£ä¸­é€å­—ç²¾ç¡®å¤åˆ¶**ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
- **original_textå¿…é¡»æ˜¯å®Œæ•´çš„ã€è¿ç»­çš„å†…å®¹å—**ï¼š
  * å¦‚æœæ˜¯ä¸€ä¸ªç« èŠ‚ï¼Œæå–ä»æ ‡é¢˜åˆ°è¯¥ç« èŠ‚ç»“æŸçš„å®Œæ•´å†…å®¹
  * å¦‚æœæ˜¯ä¸€ä¸ªæ®µè½ï¼Œæå–å®Œæ•´çš„æ®µè½ï¼ˆä¸èƒ½åªæå–å¼€å¤´æˆ–ç»“å°¾ï¼‰
  * å¦‚æœæ˜¯å¤šä¸ªå­ç« èŠ‚ï¼Œæå–å®Œæ•´çš„æ‰€æœ‰å­ç« èŠ‚
- **ç¦æ­¢ä½¿ç”¨çœç•¥å·ï¼ˆ...æˆ–â€¦ï¼‰**ï¼Œå¿…é¡»æå–å®Œæ•´æ–‡æœ¬
- **ç¦æ­¢åªæå–æ ‡é¢˜æˆ–å¼€å¤´å‡ å¥**ï¼Œå¿…é¡»æå–éœ€è¦ä¿®æ”¹çš„å®Œæ•´èŒƒå›´
- å¦‚æœæŸå¤„éœ€è¦ä¿®æ”¹çš„å†…å®¹å¤ªé•¿ï¼ˆè¶…è¿‡500è¡Œï¼‰ï¼Œå¯ä»¥æ‹†åˆ†æˆå¤šä¸ªå®Œæ•´çš„å°èŠ‚
- ä¸è¦ç”Ÿæˆä¿®æ”¹åçš„å†…å®¹ï¼Œåªæå–éœ€è¦ä¿®æ”¹çš„åŸæ–‡
- å¦‚æœæ–‡æ¡£æ— éœ€ä¿®æ”¹ï¼Œè®¾ç½®needs_modificationä¸ºfalse

**ç‰¹åˆ«æ³¨æ„**ï¼š
- âŒ é”™è¯¯ç¤ºä¾‹ï¼šåªæå–äº†æ ‡é¢˜å’Œå¼€å¤´
  ```
  "original_text": "## 3.3. Loss Function\n\næœ¬èŠ‚è¯¦ç»†é˜è¿°..."
  ```
- âœ… æ­£ç¡®ç¤ºä¾‹ï¼šæå–äº†å®Œæ•´çš„ç« èŠ‚ï¼ˆåŒ…æ‹¬æ‰€æœ‰å­ç« èŠ‚ï¼‰
  ```
  "original_text": "## 3.3. Loss Function\n\næœ¬èŠ‚è¯¦ç»†é˜è¿°...\n\n### 3.3.1. ...\n\nï¼ˆå®Œæ•´å†…å®¹ï¼‰\n\n### 3.3.2. ...\n\nï¼ˆå®Œæ•´å†…å®¹ï¼‰"
  ```

**ç¤ºä¾‹**ï¼ˆæ­£ç¡®ï¼‰:
```json
{{
  "location": "1. Introduction ç¬¬ä¸€æ®µ",
  "original_text": "ä½œç‰©äº§é‡é¢„æµ‹å¯¹ä¿éšœç²®é£Ÿå®‰å…¨ã€ä¼˜åŒ–å†œä¸šç”Ÿäº§å¸ƒå±€ä»¥åŠåˆ¶å®šç²¾å‡†å†œä¸šæ”¿ç­–å…·æœ‰æ˜¾è‘—çš„ç°å®æ„ä¹‰å’Œåº”ç”¨ä»·å€¼ã€‚"
}}
```

**ç¤ºä¾‹**ï¼ˆé”™è¯¯ï¼‰:
```json
{{
  "location": "1. Introduction ç¬¬ä¸€æ®µ",
  "original_text": "ä½œç‰©äº§é‡é¢„æµ‹å¯¹ä¿éšœç²®é£Ÿå®‰å…¨...å…·æœ‰æ˜¾è‘—æ„ä¹‰ã€‚"  // âŒ ä½¿ç”¨äº†çœç•¥å·
}}
```

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚å¦‚æœæ— æ³•è¿”å›JSONï¼Œè¯·è¿”å›ï¼š{{"needs_modification": false, "modification_points": [], "overall_guidance": "æ— æ³•åˆ†æ"}}"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£è¯„ä¼°ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææ–‡æ¡£å¹¶è¯†åˆ«éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†ã€‚"},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.3
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # æå–JSON
            if "```json" in raw_response:
                json_str = raw_response.split("```json")[1].split("```")[0].strip()
            elif "```" in raw_response:
                json_str = raw_response.split("```")[1].split("```")[0].strip()
            else:
                json_str = raw_response
            
            evaluation = json.loads(json_str)
            
            modification_points = evaluation.get("modification_points", [])
            logger.info(f"ğŸ“‹ è¯„ä¼°ç»“æœ: éœ€è¦ä¿®æ”¹ {len(modification_points)} å¤„")
            for idx, point in enumerate(modification_points, 1):
                logger.info(f"  {idx}. [{point.get('location', 'æœªçŸ¥ä½ç½®')}] {point.get('modification_type', 'æœªçŸ¥ç±»å‹')}")
            
            return evaluation
            
        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
            logger.error(f"AIåŸå§‹å“åº”: {raw_response[:500] if 'raw_response' in locals() else 'N/A'}")
            return {
                "needs_modification": False,
                "modification_points": [],
                "overall_guidance": f"è¯„ä¼°å¤±è´¥: {str(e)}"
            }
    
    async def _generate_with_react_agent(self,
                                        modification_request: str,
                                        original_content: str,
                                        evaluation: Dict,
                                        project_id: str = None) -> Dict:
        """
        ä½¿ç”¨ReactAgentä¸ºæ¯ä¸ªä¿®æ”¹ç‚¹æœé›†èµ„æ–™å¹¶ç”Ÿæˆä¿®æ”¹åçš„ç‰‡æ®µ
        
        æµç¨‹ï¼š
        1. å¯¹æ¯ä¸ªè¯„ä¼°å‡ºçš„ä¿®æ”¹ç‚¹
        2. è®©ReactAgentæœé›†ç›¸å…³èµ„æ–™
        3. ReactAgentç”Ÿæˆä¿®æ”¹åçš„ç‰‡æ®µå†…å®¹
        4. æ›¿æ¢å›åŸæ–‡
        
        Args:
            modification_request: ä¿®æ”¹è¦æ±‚
            original_content: åŸå§‹æ–‡æ¡£å†…å®¹
            evaluation: AIè¯„ä¼°ç»“æœï¼ˆåŒ…å«modification_pointsï¼‰
            
        Returns:
            {
                "content": ä¿®æ”¹åçš„æ–‡æ¡£å†…å®¹,
                "thinking_process": ReactAgentæ€è€ƒè¿‡ç¨‹,
                "search_history": ReactAgentæœç´¢å†å²
            }
        """
        # å¯¼å…¥ReactAgentï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
        from react_agent import ReactAgent
        
        modification_points = evaluation.get("modification_points", [])
        
        # å¦‚æœæ²¡æœ‰å…·ä½“çš„ä¿®æ”¹ç‚¹ï¼Œç›´æ¥è¿”å›åŸæ–‡
        if not modification_points:
            logger.warning("è¯„ä¼°ç»“æœä¸­æ²¡æœ‰å…·ä½“ä¿®æ”¹ç‚¹ï¼Œè¿”å›åŸæ–‡")
            return {
                "content": original_content,
                "thinking_process": [],
                "search_history": []
            }
        
        # åˆ›å»ºReactAgentå®ä¾‹
        agent = ReactAgent(
            max_iterations=3,
            project_id=project_id or self.project_id,
            top_k=10,
            use_refine=False
        )
        
        # ğŸš€ å¹¶è¡Œå¤„ç†æ‰€æœ‰ä¿®æ”¹ç‚¹
        logger.info(f"ğŸš€ å¹¶è¡Œå¤„ç† {len(modification_points)} ä¸ªä¿®æ”¹ç‚¹...")
        
        async def process_single_point(idx, point):
            """å¤„ç†å•ä¸ªä¿®æ”¹ç‚¹"""
            try:
                location = point.get("location", "æœªçŸ¥ä½ç½®")
                original_text_ref = point.get("original_text", "")  # è¯„ä¼°é˜¶æ®µçš„å‚è€ƒåŸæ–‡
                modification_reason = point.get("modification_reason", "")
                modification_type = point.get("modification_type", "")
                
                logger.info(f"ğŸ”„ ä¿®æ”¹ç‚¹ {idx}/{len(modification_points)}: [{location}] - {modification_type}")
                
                # æ„å»ºReactAgentçš„æœç´¢å’Œç”Ÿæˆä»»åŠ¡
                # ç®€åŒ–ç­–ç•¥ï¼šè®©ReactAgentåªç”Ÿæˆä¿®æ”¹åçš„å†…å®¹ï¼Œä¸æå–åŸæ–‡
                react_task = f"""æ ¹æ®ä»¥ä¸‹è¦æ±‚ï¼Œç”Ÿæˆä¿®æ”¹åçš„å†…å®¹ç‰‡æ®µã€‚

ã€ä¿®æ”¹è¦æ±‚ã€‘
{modification_request}

ã€ä¿®æ”¹ä½ç½®ã€‘
{location}

ã€ä¿®æ”¹ç±»å‹ã€‘
{modification_type}

ã€ä¿®æ”¹åŸå› ã€‘
{modification_reason}

ã€åŸæ–‡å‚è€ƒã€‘
{original_text_ref}

**ä»»åŠ¡**ï¼š
1. æœé›†ç›¸å…³èµ„æ–™æ¥å®Œå–„ä¿®æ”¹å†…å®¹
2. åŸºäºåŸæ–‡å’ŒRAGèµ„æ–™ï¼Œç”Ÿæˆä¿®æ”¹åçš„å†…å®¹

**è¾“å‡ºè¦æ±‚**ï¼š
- ç›´æ¥è¾“å‡ºä¿®æ”¹åçš„å®Œæ•´å†…å®¹ç‰‡æ®µ
- ä¿æŒåŸæ–‡çš„æ ¼å¼å’Œç»“æ„ï¼ˆå¦‚Markdownæ ¼å¼ï¼‰
- åŸºäºRAGæœç´¢çš„èµ„æ–™å®Œå–„ä¿®æ”¹å†…å®¹
- åªä¿®æ”¹å¿…è¦çš„éƒ¨åˆ†ï¼Œä¸è¦å¤§å¹…æ”¹å†™
- ä¸è¦é‡å¤è¾“å‡ºåŸæ–‡

ç›´æ¥è¾“å‡ºä¿®æ”¹åçš„å†…å®¹ç‰‡æ®µï¼Œä¸è¦JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚"""
                
                # ä½¿ç”¨ReactAgentç”Ÿæˆå†…å®¹
                result = await agent.run(react_task)
                
                content = result.get("content", "").strip()
                thinking = result.get("thinking_process", [])
                search_history = result.get("search_history", [])
                
                # ç›´æ¥ä½¿ç”¨ReactAgentè¿”å›çš„å†…å®¹ä½œä¸ºä¿®æ”¹åçš„æ–‡æœ¬
                # ä½¿ç”¨è¯„ä¼°é˜¶æ®µçš„original_text_refæ¥å®šä½
                final_modified = content
                final_original = original_text_ref
                
                logger.info(f"âœ… ä¿®æ”¹ç‚¹ {idx} å®Œæˆ")
                logger.info(f"   ç”Ÿæˆå†…å®¹é•¿åº¦: {len(final_modified)} å­—ç¬¦")
            
                logger.info(f"âœ… ä¿®æ”¹ç‚¹ {idx} å¤„ç†å®Œæˆ: å°†ç”¨äºå®šä½çš„åŸæ–‡é•¿åº¦ {len(final_original)} â†’ ä¿®æ”¹å {len(final_modified)} å­—ç¬¦")
                
                return {
                    "modification": {
                        "location": location,
                        "original_text": final_original,  # ä½¿ç”¨è¯„ä¼°é˜¶æ®µçš„original_text_ref
                        "modified_text": final_modified,   # ä½¿ç”¨ReactAgentç”Ÿæˆçš„ä¿®æ”¹åå†…å®¹
                        "reason": modification_reason,
                        "modification_type": modification_type
                    },
                    "thinking": {
                        "modification_point": idx,
                        "location": location,
                        "thinking_steps": thinking,
                        "generated_length": len(final_modified),
                        "used_react_original": False
                    },
                    "search_history": search_history
                }
            except Exception as e:
                logger.error(f"âŒ ä¿®æ”¹ç‚¹ {idx} [{location}] å¤„ç†å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                # è¿”å›ä¸€ä¸ªé»˜è®¤çš„ç»“æœï¼Œä¿æŒåŸæ–‡ä¸å˜
                return {
                    "modification": {
                        "location": location,
                        "original_text": original_text_ref,
                        "modified_text": original_text_ref,  # ä¿æŒåŸæ ·
                        "reason": f"å¤„ç†å¤±è´¥: {str(e)}",
                        "modification_type": modification_type
                    },
                    "thinking": {
                        "modification_point": idx,
                        "location": location,
                        "thinking_steps": [],
                        "generated_length": len(original_text_ref),
                        "used_react_original": False
                    },
                    "search_history": []
                }
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰ä¿®æ”¹ç‚¹
        tasks = [
            process_single_point(idx, point) 
            for idx, point in enumerate(modification_points, 1)
        ]
        results = await asyncio.gather(*tasks)
        
        # æ•´ç†ç»“æœ
        modifications_list = [r["modification"] for r in results]
        all_thinking_process = [r["thinking"] for r in results]
        all_search_history = []
        for r in results:
            all_search_history.extend(r["search_history"])
        
        # åº”ç”¨æ‰€æœ‰ä¿®æ”¹åˆ°åŸæ–‡
        logger.info(f"\nğŸ“ åº”ç”¨ {len(modifications_list)} å¤„ä¿®æ”¹åˆ°åŸæ–‡...")
        modified_content = self._apply_diff_modifications(original_content, modifications_list)
        
        logger.info(f"âœ… æ‰€æœ‰ä¿®æ”¹å®Œæˆ")
        logger.info(f"   åŸæ–‡: {len(original_content)} â†’ ä¿®æ”¹å: {len(modified_content)} å­—ç¬¦")
        logger.info(f"   å˜åŒ–: {len(modified_content) - len(original_content):+d} å­—ç¬¦")
        
        return {
            "content": modified_content,
            "thinking_process": all_thinking_process,
            "search_history": all_search_history
        }
    
    async def _generate_modifications_with_rag(self,
                                              modification_request: str,
                                              original_content: str,
                                              modification_points: List[Dict],
                                              reference_materials: str) -> List[Dict]:
        """
        åŸºäºRAGæœç´¢èµ„æ–™å’Œè¯„ä¼°ç»“æœï¼Œç”Ÿæˆå…·ä½“çš„ä¿®æ”¹å»ºè®®ï¼ˆJSON diffæ ¼å¼ï¼‰
        
        Args:
            modification_request: ä¿®æ”¹è¦æ±‚
            original_content: åŸå§‹æ–‡æ¡£å†…å®¹
            modification_points: è¯„ä¼°å‡ºçš„ä¿®æ”¹ç‚¹åˆ—è¡¨
            reference_materials: RAGæœç´¢åˆ°çš„å‚è€ƒèµ„æ–™
            
        Returns:
            modificationsåˆ—è¡¨: [{"location": "...", "original_text": "...", "modified_text": "...", "reason": "..."}]
        """
        # æ„å»ºä¿®æ”¹ç‚¹æ‘˜è¦
        points_summary = "\n".join([
            f"{idx}. ä½ç½®ï¼š{point.get('location', 'æœªçŸ¥')}\n"
            f"   åŸæ–‡ç‰‡æ®µï¼š{point.get('original_text', '')[:200]}...\n"
            f"   ä¿®æ”¹åŸå› ï¼š{point.get('modification_reason', '')}\n"
            f"   ä¿®æ”¹ç±»å‹ï¼š{point.get('modification_type', '')}"
            for idx, point in enumerate(modification_points, 1)
        ])
        
        prompt = f"""ä½ éœ€è¦æ ¹æ®è¯„ä¼°ç»“æœå’Œå‚è€ƒèµ„æ–™ï¼Œä¸ºæ–‡æ¡£ç”Ÿæˆå…·ä½“çš„ä¿®æ”¹å»ºè®®ã€‚

ä¿®æ”¹è¦æ±‚ï¼š
{modification_request}

è¯„ä¼°å‡ºçš„ä¿®æ”¹ç‚¹ï¼š
{points_summary}

å‚è€ƒèµ„æ–™ï¼š
{reference_materials if reference_materials else "æ— "}

åŸå§‹æ–‡æ¡£ï¼ˆéƒ¨åˆ†ï¼‰ï¼š
{original_content[:2000]}...

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºæ¯ä¸ªä¿®æ”¹ç‚¹ç”Ÿæˆå…·ä½“çš„ä¿®æ”¹å†…å®¹ã€‚ä½¿ç”¨JSONæ ¼å¼è¾“å‡ºï¼š
```json
{{
  "modifications": [
    {{
      "location": "ç« èŠ‚åç§°æˆ–ä½ç½®æè¿°",
      "original_text": "éœ€è¦æ›¿æ¢çš„åŸå§‹æ–‡æœ¬ï¼ˆä»æ–‡æ¡£ä¸­ç²¾ç¡®å¤åˆ¶ï¼‰",
      "modified_text": "ä¿®æ”¹åçš„æ–‡æœ¬ï¼ˆå¯ä»¥å‚è€ƒRAGèµ„æ–™å®Œå–„å†…å®¹ï¼‰",
      "reason": "ä¿®æ”¹åŸå› "
    }}
  ]
}}
```

**é‡è¦è¦æ±‚**ï¼š
1. original_textå¿…é¡»ä»æ–‡æ¡£ä¸­ç²¾ç¡®å¤åˆ¶ï¼Œç”¨äºå®šä½
2. modified_textåº”è¯¥å‚è€ƒRAGæœç´¢åˆ°çš„èµ„æ–™ï¼ˆå¦‚æœæœ‰ï¼‰æ¥å®Œå–„
3. ä¿æŒåŸæ–‡çš„æ ¼å¼å’Œé£æ ¼
4. åªä¿®æ”¹å¿…è¦çš„éƒ¨åˆ†ï¼Œä¸è¦å¤§å¹…æ”¹å†™

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘ï¼Œæ“…é•¿åŸºäºå‚è€ƒèµ„æ–™ç”Ÿæˆç²¾ç¡®çš„ä¿®æ”¹å»ºè®®ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=3000
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # æå–JSON
            if "```json" in raw_response:
                json_str = raw_response.split("```json")[1].split("```")[0].strip()
            elif "```" in raw_response:
                json_str = raw_response.split("```")[1].split("```")[0].strip()
            else:
                json_str = raw_response
            
            modifications_data = json.loads(json_str)
            modifications_list = modifications_data.get("modifications", [])
            
            logger.info(f"ğŸ“ ç”Ÿæˆäº† {len(modifications_list)} ä¸ªä¿®æ”¹å»ºè®®")
            for idx, mod in enumerate(modifications_list, 1):
                logger.info(f"  {idx}. [{mod.get('location', 'æœªçŸ¥')}] {mod.get('reason', '')}")
            
            return modifications_list
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆä¿®æ”¹å»ºè®®å¤±è´¥: {str(e)}")
            # é™çº§æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨è¯„ä¼°ç‚¹ä½œä¸ºä¿®æ”¹å»ºè®®
            return [{
                "location": point.get("location", "æœªçŸ¥"),
                "original_text": point.get("original_text", ""),
                "modified_text": point.get("original_text", ""),  # ä¿æŒåŸæ ·
                "reason": f"ç”Ÿæˆå¤±è´¥ï¼Œä¿æŒåŸæ ·: {str(e)}"
            } for point in modification_points]
    
    async def read_file_content(self, minio_url: str) -> str:
        """
        è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆä»MinIOï¼‰
        
        Args:
            minio_url: MinIOæ–‡ä»¶URLï¼ˆæ¥è‡ªRAG metadataï¼‰
            
        Returns:
            æ–‡ä»¶å†…å®¹
        """
        try:
            # ä»MinIOè¯»å–æ–‡ä»¶
            content = await self.kb_manager.read_file_from_minio(minio_url)
            
            if content:
                logger.info(f"æˆåŠŸä»MinIOè¯»å–æ–‡ä»¶, é•¿åº¦: {len(content)} å­—ç¬¦")
                return content
            else:
                logger.error(f"ä»MinIOè¯»å–æ–‡ä»¶å¤±è´¥: {minio_url}")
                return ""
                
        except Exception as e:
            logger.error(f"è¯»å–MinIOæ–‡ä»¶å¼‚å¸¸ {minio_url}: {str(e)}")
            return ""
    
    def _apply_diff_modifications(self, original_content: str, modifications: list) -> str:
        """
        å°†JSONæ ¼å¼çš„ä¿®æ”¹åº”ç”¨åˆ°åŸæ–‡æ¡£ï¼ˆæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…ç‰ˆæœ¬ï¼‰
        
        Args:
            original_content: åŸå§‹æ–‡æ¡£å†…å®¹
            modifications: [{"location": "...", "original_text": "...", "modified_text": "...", "reason": "..."}]
            
        Returns:
            ä¿®æ”¹åçš„æ–‡æ¡£å†…å®¹
        """
        result = original_content
        applied_count = 0
        failed_mods = []
        skipped_duplicates = []
        
        # æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºæ¯”è¾ƒ
        def normalize_text(text):
            """æ ‡å‡†åŒ–æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€ç»Ÿä¸€æ¢è¡Œã€å»é™¤çœç•¥å·"""
            # å»é™¤çœç•¥å·
            text = text.replace('...', ' ')
            text = text.replace('â€¦', ' ')
            # ç»Ÿä¸€ç©ºç™½å­—ç¬¦
            text = ' '.join(text.split())
            return text.strip()
        
        def fuzzy_find_in_content(search_text, content, threshold=0.8):
            """
            åœ¨å†…å®¹ä¸­æ¨¡ç³ŠæŸ¥æ‰¾æ–‡æœ¬
            
            Args:
                search_text: è¦æŸ¥æ‰¾çš„æ–‡æœ¬ï¼ˆå¯èƒ½ä¸ç²¾ç¡®ï¼‰
                content: å†…å®¹
                threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
                
            Returns:
                (æ‰¾åˆ°çš„æ–‡æœ¬, èµ·å§‹ä½ç½®) æˆ– (None, -1)
            """
            search_normalized = normalize_text(search_text)
            search_words = search_normalized.split()
            
            # å¦‚æœsearch_textå¤ªçŸ­ï¼Œç›´æ¥ç²¾ç¡®æŸ¥æ‰¾
            if len(search_normalized) < 20:
                if search_text in content:
                    return search_text, content.find(search_text)
                return None, -1
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = content.split('\n\n')
            
            for p_idx, paragraph in enumerate(paragraphs):
                para_normalized = normalize_text(paragraph)
                para_words = para_normalized.split()
                
                # æ£€æŸ¥å…³é”®è¯åŒ¹é…åº¦
                if len(search_words) > 0:
                    # è®¡ç®—æœ‰å¤šå°‘ä¸ªå…³é”®è¯å‡ºç°åœ¨æ®µè½ä¸­
                    matched_words = sum(1 for word in search_words if word in para_words)
                    similarity = matched_words / len(search_words)
                    
                    if similarity >= threshold:
                        # æ‰¾åˆ°åŒ¹é…çš„æ®µè½
                        return paragraph, content.find(paragraph)
            
            # å°è¯•æŒ‰å¥å­æŸ¥æ‰¾ï¼ˆå¤„ç†è·¨æ®µè½çš„æƒ…å†µï¼‰
            sentences = content.replace('\n\n', '\n').split('\n')
            for sent in sentences:
                sent_normalized = normalize_text(sent)
                sent_words = sent_normalized.split()
                
                if len(search_words) > 0:
                    matched_words = sum(1 for word in search_words if word in sent_words)
                    similarity = matched_words / len(search_words)
                    
                    if similarity >= threshold:
                        return sent, content.find(sent)
            
            return None, -1
        
        # å»é‡ + æ™ºèƒ½æ‰©å±•åŸæ–‡æå–
        seen_originals = set()
        deduplicated_mods = []
        for mod in modifications:
            original_text = mod.get("original_text", "").strip()
            modified_text = mod.get("modified_text", "").strip()
            location = mod.get("location", "æœªçŸ¥")
            
            # ğŸ”§ æ™ºèƒ½æ£€æµ‹ï¼šå¦‚æœmodified_textæ¯”original_texté•¿å¾ˆå¤šï¼Œè¯´æ˜æå–ä¸å®Œæ•´
            if len(modified_text) > len(original_text) * 2 and len(modified_text) > 500:
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¿®æ”¹ç‚¹ [{location}] çš„original_textå¯èƒ½ä¸å®Œæ•´")
                logger.warning(f"   åŸæ–‡é•¿åº¦: {len(original_text)}, ä¿®æ”¹åé•¿åº¦: {len(modified_text)}")
                logger.warning(f"   å°è¯•ä»æ–‡æ¡£ä¸­æ‰©å±•æå–èŒƒå›´...")
                
                # å°è¯•æ™ºèƒ½æ‰©å±•ï¼šæ‰¾åˆ°åŒ…å«original_textçš„æ›´å¤§æ®µè½
                expanded_text = self._expand_original_text(original_content, original_text)
                if expanded_text and len(expanded_text) > len(original_text):
                    logger.info(f"   âœ… æ‰©å±•æˆåŠŸ: {len(original_text)} â†’ {len(expanded_text)} å­—ç¬¦")
                    original_text = expanded_text
                    mod["original_text"] = original_text
            
            original_normalized = normalize_text(original_text)
            
            if original_text and original_normalized not in seen_originals:
                seen_originals.add(original_normalized)
                deduplicated_mods.append(mod)
            elif original_text:
                logger.info(f"âš ï¸ è·³è¿‡é‡å¤çš„ä¿®æ”¹ç‚¹: {location}")
                logger.info(f"   å†…å®¹: {original_text[:60]}...")
                skipped_duplicates.append(location)
        
        if skipped_duplicates:
            logger.info(f"ğŸ”„ å»é‡: è·³è¿‡äº† {len(skipped_duplicates)} ä¸ªé‡å¤çš„ä¿®æ”¹ç‚¹")
        
        # æŒ‰é¡ºåºåº”ç”¨æ¯ä¸ªä¿®æ”¹
        for idx, mod in enumerate(deduplicated_mods, 1):
            original_text = mod.get("original_text", "").strip()
            modified_text = mod.get("modified_text", "").strip()
            location = mod.get("location", "æœªæŒ‡å®šä½ç½®")
            reason = mod.get("reason", "")
            
            if not original_text:
                logger.warning(f"âš ï¸ ä¿®æ”¹ #{idx} [{location}]: ç¼ºå°‘original_text")
                failed_mods.append(f"{location} (ç¼ºå°‘åŸæ–‡)")
                continue
            
            # æ ‡å‡†åŒ–æ¯”è¾ƒ
            original_normalized = normalize_text(original_text)
            modified_normalized = normalize_text(modified_text)
            
            # æ£€æµ‹æ˜¯å¦çœŸçš„æœ‰ä¿®æ”¹
            if original_normalized == modified_normalized:
                logger.info(f"â­ï¸  ä¿®æ”¹ #{idx} [{location}]: å†…å®¹å®è´¨æœªå˜åŒ–ï¼Œè·³è¿‡")
                logger.info(f"   åŸæ–‡: {original_text[:60]}...")
                continue
            
            # ğŸš¨ é˜²æ­¢é‡å¤ï¼šæ™ºèƒ½æ£€æµ‹é‡å¤æ¨¡å¼
            
            # æ£€æµ‹1ï¼šmodified_text åŒ…å« original_text
            if original_text in modified_text and modified_text != original_text:
                if modified_text.startswith(original_text):
                    logger.warning(f"âš ï¸ ä¿®æ”¹ #{idx} [{location}]: æ£€æµ‹åˆ°ä¿®æ”¹å†…å®¹åŒ…å«åŸæ–‡ï¼ˆå‰ç½®ï¼‰")
                    logger.warning(f"   è·³è¿‡ä»¥é˜²æ­¢é‡å¤")
                    continue
                elif modified_text.endswith(original_text):
                    logger.warning(f"âš ï¸ ä¿®æ”¹ #{idx} [{location}]: æ£€æµ‹åˆ°ä¿®æ”¹å†…å®¹åŒ…å«åŸæ–‡ï¼ˆåç½®ï¼‰")
                    logger.warning(f"   è·³è¿‡ä»¥é˜²æ­¢é‡å¤")
                    continue
            
            # æ£€æµ‹2ï¼šæ£€æŸ¥æ›¿æ¢åæ˜¯å¦ä¼šå¯¼è‡´æ®µè½é‡å¤
            # æå– modified_text çš„å‰100å­—ç¬¦ä½œä¸ºç‰¹å¾
            modified_signature = modified_text[:100].strip()
            if modified_signature and modified_signature in result:
                # æ£€æŸ¥è¿™ä¸ªç­¾åæ˜¯å¦å·²ç»åœ¨æ–‡æ¡£ä¸­ï¼ˆä¸æ˜¯æ¥è‡ªoriginal_textï¼‰
                if modified_signature not in original_text:
                    logger.warning(f"âš ï¸ ä¿®æ”¹ #{idx} [{location}]: ä¿®æ”¹å†…å®¹çš„å¼€å¤´å·²å­˜åœ¨äºæ–‡æ¡£ä¸­")
                    logger.warning(f"   ç‰¹å¾: {modified_signature[:50]}...")
                    logger.warning(f"   å¯èƒ½å¯¼è‡´é‡å¤ï¼Œè·³è¿‡")
                    continue
            
            # æ–¹æ³•1: ç²¾ç¡®åŒ¹é…
            if original_text in result:
                # æ£€æŸ¥æ›¿æ¢åæ˜¯å¦ä¼šå¯¼è‡´é‡å¤
                temp_result = result.replace(original_text, modified_text, 1)
                
                # æ£€æµ‹æ˜¯å¦ä¼šäº§ç”Ÿè¿ç»­é‡å¤çš„å†…å®¹
                if modified_text in result and modified_text != original_text:
                    logger.warning(f"âš ï¸ ä¿®æ”¹ #{idx} [{location}]: ä¿®æ”¹åçš„å†…å®¹å·²å­˜åœ¨äºæ–‡æ¡£ä¸­")
                    logger.warning(f"   è·³è¿‡ä»¥é˜²æ­¢é‡å¤")
                    logger.warning(f"   ä¿®æ”¹å†…å®¹: {modified_text[:60]}...")
                    continue
                
                result = temp_result
                applied_count += 1
                logger.info(f"âœ… ä¿®æ”¹ #{idx} [{location}] (ç²¾ç¡®åŒ¹é…)")
                logger.info(f"   {len(original_text)} å­—ç¬¦ â†’ {len(modified_text)} å­—ç¬¦")
                if reason:
                    logger.info(f"   åŸå› : {reason}")
            else:
                # æ–¹æ³•2: æ¨¡ç³ŠåŒ¹é…
                logger.info(f"ğŸ” å°è¯•æ¨¡ç³ŠåŒ¹é…ä¿®æ”¹ç‚¹ #{idx} [{location}]...")
                found_text, pos = fuzzy_find_in_content(original_text, result, threshold=0.7)
                
                if found_text and pos != -1:
                    # æ‰¾åˆ°äº†åŒ¹é…çš„æ–‡æœ¬
                    logger.info(f"âœ… ä¿®æ”¹ #{idx} [{location}] (æ¨¡ç³ŠåŒ¹é…ï¼Œç›¸ä¼¼åº¦>=70%)")
                    logger.info(f"   æ‰¾åˆ°çš„æ–‡æœ¬: {found_text[:80]}...")
                    
                    # æ›¿æ¢æ‰¾åˆ°çš„æ–‡æœ¬
                    result = result.replace(found_text, modified_text, 1)
                    applied_count += 1
                    if reason:
                        logger.info(f"   åŸå› : {reason}")
                else:
                    # æ–¹æ³•3: é™ä½é˜ˆå€¼å†è¯•ä¸€æ¬¡
                    found_text, pos = fuzzy_find_in_content(original_text, result, threshold=0.5)
                    
                    if found_text and pos != -1:
                        logger.info(f"âœ… ä¿®æ”¹ #{idx} [{location}] (ä½ç›¸ä¼¼åº¦åŒ¹é…ï¼Œç›¸ä¼¼åº¦>=50%)")
                        logger.info(f"   æ‰¾åˆ°çš„æ–‡æœ¬: {found_text[:80]}...")
                        logger.warning(f"   âš ï¸ æ³¨æ„ï¼šæ­¤åŒ¹é…ç›¸ä¼¼åº¦è¾ƒä½ï¼Œè¯·æ£€æŸ¥ç»“æœ")
                        
                        # æ›¿æ¢æ‰¾åˆ°çš„æ–‡æœ¬
                        result = result.replace(found_text, modified_text, 1)
                        applied_count += 1
                    else:
                        # å®Œå…¨æ— æ³•å®šä½
                        logger.warning(f"âŒ ä¿®æ”¹ #{idx} [{location}]: æ— æ³•å®šä½ï¼ˆå³ä½¿ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰")
                        logger.warning(f"   æŸ¥æ‰¾æ–‡æœ¬: {original_text[:100]}...")
                        logger.warning(f"   æç¤ºï¼šAIæå–çš„åŸæ–‡å¯èƒ½ä¸å‡†ç¡®ï¼ŒåŒ…å«çœç•¥å·æˆ–æ ¼å¼é—®é¢˜")
                        failed_mods.append(location)
        
        logger.info(f"\nğŸ“Š ä¿®æ”¹ç»Ÿè®¡:")
        logger.info(f"   æ€»ä¿®æ”¹ç‚¹: {len(modifications)}")
        logger.info(f"   å»é‡å: {len(deduplicated_mods)}")
        logger.info(f"   æˆåŠŸåº”ç”¨: {applied_count}")
        logger.info(f"   å¤±è´¥: {len(failed_mods)}")
        
        if failed_mods:
            logger.warning(f"âš ï¸ æœªåº”ç”¨çš„ä¿®æ”¹: {', '.join(failed_mods)}")
        
        # ğŸ”§ åå¤„ç†ï¼šæ£€æµ‹å¹¶ç§»é™¤é‡å¤çš„æ®µè½
        result = self._remove_duplicate_paragraphs(result)
        
        return result
    
    def _remove_duplicate_paragraphs(self, content: str) -> str:
        """
        æ£€æµ‹å¹¶ç§»é™¤æ–‡æ¡£ä¸­é‡å¤çš„æ®µè½
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            
        Returns:
            å»é‡åçš„æ–‡æ¡£å†…å®¹
        """
        paragraphs = content.split('\n\n')
        seen_paragraphs = {}
        unique_paragraphs = []
        removed_count = 0
        
        for idx, para in enumerate(paragraphs):
            para_normalized = para.strip()
            if not para_normalized:
                unique_paragraphs.append(para)
                continue
            
            # ä½¿ç”¨æ®µè½çš„å‰100å­—ç¬¦ä½œä¸ºç­¾å
            signature = para_normalized[:100]
            
            if signature in seen_paragraphs:
                # å‘ç°é‡å¤æ®µè½
                prev_idx = seen_paragraphs[signature]
                logger.warning(f"ğŸ”„ æ£€æµ‹åˆ°é‡å¤æ®µè½ (ä½ç½® {idx} ä¸ {prev_idx})")
                logger.warning(f"   å†…å®¹: {signature[:60]}...")
                removed_count += 1
                # è·³è¿‡è¿™ä¸ªé‡å¤æ®µè½
                continue
            else:
                seen_paragraphs[signature] = idx
                unique_paragraphs.append(para)
        
        if removed_count > 0:
            logger.info(f"âœ… ç§»é™¤äº† {removed_count} ä¸ªé‡å¤æ®µè½")
        
        return '\n\n'.join(unique_paragraphs)
    
    def _expand_original_text(self, document: str, partial_text: str) -> str:
        """
        æ™ºèƒ½æ‰©å±•åŸæ–‡æå–èŒƒå›´
        
        å¦‚æœAIåªæå–äº†ç« èŠ‚çš„å¼€å¤´ï¼Œå°è¯•æå–å®Œæ•´çš„ç« èŠ‚
        
        Args:
            document: å®Œæ•´æ–‡æ¡£
            partial_text: éƒ¨åˆ†æå–çš„åŸæ–‡
            
        Returns:
            æ‰©å±•åçš„å®Œæ•´åŸæ–‡
        """
        if not partial_text or partial_text not in document:
            return partial_text
        
        # æ‰¾åˆ°partial_textåœ¨æ–‡æ¡£ä¸­çš„ä½ç½®
        start_pos = document.find(partial_text)
        if start_pos == -1:
            return partial_text
        
        # æ£€æµ‹partial_textæ˜¯å¦ä»¥æ ‡é¢˜å¼€å¤´ï¼ˆ##, ###ç­‰ï¼‰
        if partial_text.startswith('#'):
            # æå–æ ‡é¢˜çº§åˆ«
            title_level = len(partial_text.split()[0])  # è®¡ç®—#çš„æ•°é‡
            
            # ä»start_poså¼€å§‹ï¼Œæ‰¾åˆ°ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§çš„æ ‡é¢˜
            end_pos = start_pos + len(partial_text)
            remaining_doc = document[end_pos:]
            
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜
            lines = remaining_doc.split('\n')
            for i, line in enumerate(lines):
                if line.strip().startswith('#'):
                    # è®¡ç®—è¿™ä¸ªæ ‡é¢˜çš„çº§åˆ«
                    current_level = len(line.strip().split()[0]) if line.strip().split() else 0
                    if current_level <= title_level:
                        # æ‰¾åˆ°äº†åŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜ï¼Œåœ¨è¿™é‡Œæˆªæ–­
                        expanded_text = document[start_pos:end_pos + sum(len(l) + 1 for l in lines[:i])]
                        return expanded_text.strip()
            
            # å¦‚æœæ²¡æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ï¼Œæå–åˆ°æ–‡æ¡£æœ«å°¾ï¼ˆä½†é™åˆ¶åœ¨5000å­—ç¬¦å†…ï¼‰
            expanded_text = document[start_pos:start_pos + len(partial_text) + 5000]
            return expanded_text.strip()
        
        # å¦‚æœä¸æ˜¯æ ‡é¢˜å¼€å¤´ï¼Œå°è¯•æ‰©å±•åˆ°æ®µè½ç»“æŸ
        end_pos = start_pos + len(partial_text)
        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªç©ºè¡Œï¼ˆæ®µè½ç»“æŸï¼‰
        next_double_newline = document.find('\n\n', end_pos)
        if next_double_newline != -1 and next_double_newline - start_pos < 3000:
            return document[start_pos:next_double_newline].strip()
        
        return partial_text
    
    def _generate_diff_summary(self, original: str, modified: str) -> str:
        """ç”Ÿæˆç®€å•çš„diffæ‘˜è¦"""
        orig_lines = original.split('\n')
        mod_lines = modified.split('\n')
        
        added = len(mod_lines) - len(orig_lines)
        
        # ç®€å•ç»Ÿè®¡å˜åŒ–
        return f"è¡Œæ•°å˜åŒ–: {added:+d}ï¼ŒåŸ{len(orig_lines)}è¡Œ â†’ æ–°{len(mod_lines)}è¡Œ"

