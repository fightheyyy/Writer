"""æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥ä¸ä¿®æ”¹æ¨¡å— - åŸºäºå¤–éƒ¨RAGç³»ç»Ÿ"""
import json
from typing import List, Dict, Set
from pathlib import Path
from openai import AsyncOpenAI
import config
from rag_tool import RAGTool
from knowledge_base import KnowledgeBaseManager
from logger import get_logger

logger = get_logger(__name__)


class ConsistencyChecker:
    """æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥å™¨ - åˆ©ç”¨å¤–éƒ¨RAGç³»ç»Ÿ"""
    
    def __init__(self, api_key: str = None, base_url: str = None, model: str = None):
        self.model = model or config.MODEL_NAME
        self.client = AsyncOpenAI(
            api_key=api_key or config.OPENROUTER_API_KEY,
            base_url=base_url or config.OPENROUTER_BASE_URL
        )
        self.rag_tool = RAGTool()
        self.kb_manager = KnowledgeBaseManager()
    
    async def find_related_documents(self, 
                                     modification_point: str,
                                     project_id: str,
                                     current_file: str = None,
                                     top_k: int = 10) -> Dict:
        """
        æŸ¥æ‰¾ä¸ä¿®æ”¹ç‚¹ç›¸å…³çš„æ‰€æœ‰æ–‡æ¡£
        
        Args:
            modification_point: ä¿®æ”¹çš„å†…å®¹ç‚¹ï¼ˆå¦‚"æ—©å­£åˆ†ç±»"ï¼‰
            project_id: é¡¹ç›®ID
            current_file: å½“å‰æ­£åœ¨ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œç”¨äºæ’é™¤ï¼‰
            top_k: RAGå¬å›æ•°é‡
            
        Returns:
            {
                "related_files": {
                    "file_path1": [chunk1, chunk2, ...],
                    "file_path2": [chunk3, ...]
                },
                "total_files": int,
                "total_chunks": int
            }
        """
        logger.info(f"æŸ¥æ‰¾ä¸ '{modification_point}' ç›¸å…³çš„æ–‡æ¡£...")
        
        # è°ƒç”¨RAGæ£€ç´¢ï¼Œä½¿ç”¨metadata_filterç­›é€‰file_chunk
        search_result = await self.rag_tool.search(
            query=modification_point,
            project_id=project_id,
            top_k=top_k,
            use_refine=False,
            metadata_filter={"content_type": "file_chunk"}
        )
        
        if not search_result["success"] or not search_result["data"]:
            logger.warning("RAGæ£€ç´¢æœªè¿”å›ç»“æœ")
            return {
                "related_files": {},
                "total_files": 0,
                "total_chunks": 0
            }
        
        data = search_result["data"]
        
        # RAGè¿”å›çš„æ•°æ®å¯èƒ½æœ‰å¤šç§ç»“æ„ï¼Œéœ€è¦çµæ´»å¤„ç†
        all_chunks = []
        
        # æ–¹å¼1: ç›´æ¥çš„bundlesæ•°ç»„ï¼ˆæ¯ä¸ªbundleåŒ…å«conversations/factsï¼‰
        if data.get("bundles"):
            for bundle in data["bundles"]:
                # ä»conversationsä¸­æå–
                for conv in bundle.get("conversations", []):
                    all_chunks.append({
                        "content": conv.get("text", ""),
                        "score": conv.get("score", 1.0),
                        "metadata": conv.get("metadata", {})
                    })
                
                # ä»factsä¸­æå–ï¼ˆä¹Ÿå¯èƒ½åŒ…å«ç›¸å…³ä¿¡æ¯ï¼‰
                for fact in bundle.get("facts", []):
                    all_chunks.append({
                        "content": fact.get("content", ""),
                        "score": fact.get("score", 1.0),
                        "metadata": fact.get("metadata", {})
                    })
            
            if all_chunks:
                logger.info(f"ä»bundlesä¸­æå–åˆ° {len(all_chunks)} ä¸ªchunks")
        
        # æ–¹å¼2: short_term_memoryæ ¼å¼ï¼ˆæ—§ç‰ˆRAGï¼‰
        elif data.get("short_term_memory"):
            short_term_memory = data["short_term_memory"]
            
            # ä»conversationsæå–
            for conv in short_term_memory.get("conversations", []):
                all_chunks.append({
                    "content": conv.get("text", ""),
                    "score": 1.0,
                    "metadata": conv.get("metadata", {})
                })
            
            # ä»factsæå–
            for fact in short_term_memory.get("facts", []):
                all_chunks.append({
                    "content": fact.get("content", ""),
                    "score": 1.0,
                    "metadata": fact.get("metadata", {})
                })
            
            if all_chunks:
                logger.info(f"ä»short_term_memoryä¸­æå–åˆ° {len(all_chunks)} ä¸ªchunks")
        
        # ä½¿ç”¨æå–åˆ°çš„chunks
        bundles = all_chunks
        
        # æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„chunks
        related_files = {}
        for i, bundle in enumerate(bundles):
            # ä»bundleä¸­æå–æ–‡ä»¶è·¯å¾„å’Œå†…å®¹
            metadata = bundle.get("metadata", {})
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„å­—æ®µåï¼ˆä¸åŒRAGç‰ˆæœ¬å¯èƒ½ä½¿ç”¨ä¸åŒå­—æ®µï¼‰
            file_identifier = (
                metadata.get("file_path") or          # ä¼˜å…ˆä½¿ç”¨file_path
                metadata.get("source_identifier") or  # å…¶æ¬¡source_identifier
                metadata.get("minio_url") or          # ç„¶åminio_url
                metadata.get("source") or             # æœ€åsource
                "unknown"
            )
            
            # è°ƒè¯•ï¼šè¾“å‡ºå‰2ä¸ªchunkçš„metadata
            if i < 2:
                logger.info(f"Chunk {i} - å¯ç”¨å­—æ®µ: {list(metadata.keys())}")
                logger.info(f"Chunk {i} - æå–çš„file_identifier: {file_identifier}")
            
            # è·³è¿‡æ— æ•ˆURL
            if file_identifier == "unknown" or not file_identifier.startswith("http"):
                logger.warning(f"è·³è¿‡æ— æ•ˆçš„file_identifier: {file_identifier} (metadata keys: {list(metadata.keys())})")
                continue
            
            # è·³è¿‡å½“å‰æ­£åœ¨ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            if current_file and file_identifier == current_file:
                continue
            
            if file_identifier not in related_files:
                related_files[file_identifier] = []
            
            related_files[file_identifier].append({
                "content": bundle.get("content", ""),
                "score": bundle.get("score", 0),
                "metadata": metadata
            })
        
        if related_files:
            logger.info(f"æ–‡ä»¶æ ‡è¯†ç¬¦ç¤ºä¾‹: {list(related_files.keys())[:2]}")
        
        logger.info(f"æ‰¾åˆ° {len(related_files)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œå…± {len(bundles)} ä¸ªchunks")
        
        return {
            "related_files": related_files,
            "total_files": len(related_files),
            "total_chunks": len(bundles)
        }
    
    async def analyze_consistency(self,
                                  modification_request: str,
                                  current_file_content: str,
                                  related_files_content: Dict[str, str]) -> Dict:
        """
        åˆ†ææ–‡æ¡£é—´çš„ä¸€è‡´æ€§ï¼Œåˆ¤æ–­å“ªäº›æ–‡æ¡£éœ€è¦åŒæ­¥ä¿®æ”¹
        
        Args:
            modification_request: ç”¨æˆ·çš„ä¿®æ”¹è¦æ±‚
            current_file_content: å½“å‰æ–‡ä»¶å†…å®¹ï¼ˆå¯é€‰ï¼Œå¯èƒ½ä¸ºNoneï¼‰
            related_files_content: {file_path: file_content}
            
        Returns:
            {
                "needs_modification": [file_path1, file_path2, ...],
                "modification_type": str,
                "consistency_analysis": str,
                "global_consistency_required": bool
            }
        """
        # å¦‚æœæ²¡æœ‰å…¶ä»–æ–‡ä»¶ï¼Œç›´æ¥è¿”å›éœ€è¦ä¿®æ”¹
        if not related_files_content:
            return {
                "needs_modification": [],
                "modification_type": "æ–‡æ¡£ä¿®æ”¹",
                "consistency_analysis": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
                "global_consistency_required": False
            }
        
        # æ„å»ºåˆ†æprompt
        files_summary = []
        for file_path, content in list(related_files_content.items())[:5]:  # æœ€å¤šåˆ†æ5ä¸ªæ–‡ä»¶
            # ä»è·¯å¾„æå–æ–‡ä»¶å
            file_name = file_path.split('/')[-1] if '/' in file_path else file_path.split('\\')[-1]
            files_summary.append(
                f"æ–‡ä»¶: {file_name}\n"
                f"å†…å®¹é¢„è§ˆ: {content[:300]}...\n"
            )
        
        # å¦‚æœæä¾›äº†å½“å‰æ–‡ä»¶å†…å®¹ï¼ŒåŒ…å«åœ¨åˆ†æä¸­
        current_file_section = ""
        if current_file_content:
            current_file_section = f"""
å½“å‰æ–‡ä»¶å†…å®¹é¢„è§ˆ:
{current_file_content[:500]}...
"""
        
        analysis_prompt = f"""ä½ éœ€è¦åˆ†æä»¥ä¸‹ä¿®æ”¹éœ€æ±‚å¯¹æ–‡æ¡£çš„å½±å“ã€‚

ä¿®æ”¹è¦æ±‚:
{modification_request}
{current_file_section}
ç›¸å…³æ–‡æ¡£:
{chr(10).join(files_summary)}

è¯·åˆ†æ:
1. æ ¹æ®ä¿®æ”¹è¦æ±‚ï¼Œå“ªäº›æ–‡æ¡£éœ€è¦ä¿®æ”¹ï¼Ÿ
2. ä¿®æ”¹ç±»å‹æ˜¯ä»€ä¹ˆï¼ˆæœ¯è¯­ç»Ÿä¸€/æ•°æ®æ›´æ–°/æ–¹æ³•æ”¹è¿›ç­‰ï¼‰ï¼Ÿ
3. ä¸ºä»€ä¹ˆè¿™äº›æ–‡æ¡£éœ€è¦ä¿®æ”¹ï¼Ÿ

ä»¥JSONæ ¼å¼è¿”å›:
{{
  "needs_modification": ["file1.md", "file2.md"],  // éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨
  "modification_type": "æœ¯è¯­ç»Ÿä¸€/è§‚ç‚¹è°ƒæ•´/æ•°æ®æ›´æ–°ç­‰",
  "consistency_analysis": "è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›æ–‡æ¡£éœ€è¦ä¿®æ”¹",
  "global_consistency_required": true/false
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            logger.info("åˆ†ææ–‡æ¡£ä¸€è‡´æ€§...")
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ä¸€è‡´æ€§åˆ†æå¸ˆã€‚"},
                    {"role": "user", "content": analysis_prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            
            content = response.choices[0].message.content.strip()
            
            # æå–JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            analysis = json.loads(content)
            logger.info(f"ä¸€è‡´æ€§åˆ†æå®Œæˆ: {json.dumps(analysis, ensure_ascii=False)}")
            return analysis
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
            # é»˜è®¤æ‰€æœ‰ç›¸å…³æ–‡ä»¶éƒ½éœ€è¦ä¿®æ”¹
            return {
                "needs_modification": list(related_files_content.keys()),
                "modification_type": "ä¸€è‡´æ€§ä¿®æ”¹",
                "consistency_analysis": "é»˜è®¤å…¨éƒ¨åŒæ­¥ä¿®æ”¹",
                "global_consistency_required": True
            }
    
    async def generate_modifications(self,
                                    modification_request: str,
                                    current_modification: str,
                                    files_to_modify: Dict[str, str]) -> List[Dict]:
        """
        ä¸ºéœ€è¦ä¿®æ”¹çš„æ–‡æ¡£ç”Ÿæˆä¿®æ”¹ç‰ˆæœ¬
        
        Args:
            modification_request: ä¿®æ”¹è¦æ±‚
            current_modification: å½“å‰æ–‡ä»¶çš„ä¿®æ”¹ç¤ºä¾‹
            files_to_modify: {file_path: file_content}
            
        Returns:
            [
                {
                    "file_path": str,
                    "original_content": str,
                    "modified_content": str,
                    "diff_summary": str
                }
            ]
        """
        logger.info(f"ç”Ÿæˆ {len(files_to_modify)} ä¸ªæ–‡æ¡£çš„ä¿®æ”¹ç‰ˆæœ¬...")
        
        modifications = []
        
        for file_path, original_content in files_to_modify.items():
            modified = await self._modify_single_file(
                modification_request,
                current_modification,
                file_path,
                original_content
            )
            modifications.append(modified)
        
        return modifications
    
    async def _modify_single_file(self,
                                  modification_request: str,
                                  current_modification: str,
                                  minio_url: str,
                                  original_content: str) -> Dict:
        """ä¿®æ”¹å•ä¸ªæ–‡ä»¶"""
        
        # ä»URLæå–æ–‡ä»¶å
        file_name = minio_url.split('/')[-1] if '/' in minio_url else minio_url
        
        # æ„å»ºpromptï¼Œå¦‚æœæœ‰å‚è€ƒä¿®æ”¹å°±åŒ…å«ï¼Œå¦åˆ™ç›´æ¥æ ¹æ®è¦æ±‚ä¿®æ”¹
        if current_modification:
            reference_section = f"""
å‚è€ƒä¿®æ”¹ç¤ºä¾‹ï¼ˆä¿æŒä¸€è‡´çš„ä¿®æ”¹é£æ ¼ï¼‰:
{current_modification[:500]}...
"""
        else:
            reference_section = ""
        
        # ğŸš€ ç»Ÿä¸€ä½¿ç”¨JSON diffæ ¼å¼ - æ— tokené™åˆ¶ï¼Œé«˜æ•ˆç²¾å‡†
        prompt = f"""ä½ éœ€è¦åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼Œæ‰¾å‡ºæ‰€æœ‰éœ€è¦ä¿®æ”¹çš„åœ°æ–¹ã€‚

ä¿®æ”¹è¦æ±‚:
{modification_request}
{reference_section}
å¾…ä¿®æ”¹æ–‡ä»¶: {file_name}
æ–‡ä»¶å†…å®¹:
{original_content}

è¦æ±‚:
1. **å…¨å±€åˆ†æ**: æ‰¾å‡ºæ–‡æ¡£ä¸­æ‰€æœ‰ä¸"{modification_request}"ç›¸å…³çš„å†…å®¹
2. **ç²¾ç¡®å®šä½**: æå–éœ€è¦ä¿®æ”¹çš„åŸå§‹æ–‡æœ¬ç‰‡æ®µï¼ˆå¿…é¡»ä¸æ–‡æ¡£ä¸­çš„æ–‡æœ¬å®Œå…¨ä¸€è‡´ï¼‰
3. **å®Œæ•´ä¿®æ”¹**: ç»™å‡ºä¿®æ”¹åçš„æ–‡æœ¬
4. **ä¿æŒæ ¼å¼**: ä¿ç•™åŸæœ‰çš„Markdownæ ¼å¼

**è¾“å‡ºæ ¼å¼**: å¿…é¡»ä½¿ç”¨ä»¥ä¸‹JSONæ ¼å¼ï¼š
```json
{{
  "modifications": [
    {{
      "location": "ç« èŠ‚åç§°æˆ–ä½ç½®æè¿°",
      "original_text": "éœ€è¦æ›¿æ¢çš„åŸå§‹æ–‡æœ¬ï¼ˆå¿…é¡»å®Œå…¨åŒ¹é…ï¼‰",
      "modified_text": "ä¿®æ”¹åçš„æ–‡æœ¬",
      "reason": "ä¿®æ”¹åŸå› "
    }}
  ]
}}
```

**é‡è¦è§„åˆ™**:
- åªè¾“å‡ºéœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†ï¼Œä¸è¦è¾“å‡ºæ•´ä¸ªæ–‡æ¡£
- original_textå¿…é¡»ä»æ–‡æ¡£ä¸­ç²¾ç¡®å¤åˆ¶ï¼Œç”¨äºå®šä½å’Œæ›¿æ¢
- å¦‚æœéœ€è¦ä¿®æ”¹å¤šå¤„ï¼Œåˆ—å‡ºæ‰€æœ‰ä¿®æ”¹é¡¹
- å¯ä»¥æå–è¾ƒé•¿çš„æ–‡æœ¬ç‰‡æ®µä»¥ç¡®ä¿å”¯ä¸€æ€§

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚"""

        try:
            logger.info(f"ğŸ” åˆ†ææ–‡æ¡£ä¿®æ”¹: {file_name} (åŸæ–‡: {len(original_content)} å­—ç¬¦)")
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘ï¼Œæ“…é•¿ç²¾ç¡®å®šä½å’Œä¿®æ”¹æ–‡æ¡£å†…å®¹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3  # é™ä½æ¸©åº¦ï¼Œæé«˜ç²¾ç¡®åº¦
                # ä¸è®¾ç½®max_tokensï¼ŒJSON diffæ ¼å¼ä¸ä¼šè¶…é™
            )
            
            raw_response = response.choices[0].message.content.strip()
            finish_reason = response.choices[0].finish_reason
            
            # è§£æJSON diffæ ¼å¼
            try:
                # æå–JSON
                if "```json" in raw_response:
                    json_str = raw_response.split("```json")[1].split("```")[0].strip()
                elif "```" in raw_response:
                    json_str = raw_response.split("```")[1].split("```")[0].strip()
                else:
                    json_str = raw_response
                
                modifications_data = json.loads(json_str)
                modifications_list = modifications_data.get("modifications", [])
                
                if not modifications_list:
                    logger.info(f"â„¹ï¸ AIè®¤ä¸ºæ–‡æ¡£ {file_name} æ— éœ€ä¿®æ”¹")
                    modified_content = original_content
                    diff_summary = "æ— éœ€ä¿®æ”¹"
                else:
                    # åº”ç”¨æ‰€æœ‰ä¿®æ”¹åˆ°åŸæ–‡æ¡£
                    modified_content = self._apply_diff_modifications(
                        original_content, 
                        modifications_list
                    )
                    
                    diff_summary = f"âœ… åº”ç”¨äº† {len(modifications_list)} å¤„ä¿®æ”¹"
                    logger.info(f"âœ… ä¿®æ”¹å®Œæˆ: {file_name}")
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
                logger.error(f"åŸå§‹å“åº”: {raw_response[:500]}...")
                modified_content = original_content
                diff_summary = f"âŒ JSONè§£æå¤±è´¥ï¼Œæ–‡æ¡£æœªä¿®æ”¹"
            except Exception as e:
                logger.error(f"âŒ åº”ç”¨ä¿®æ”¹å¤±è´¥: {str(e)}")
                modified_content = original_content
                diff_summary = f"âŒ ä¿®æ”¹åº”ç”¨å¤±è´¥: {str(e)}"
            
            return {
                "file_path": minio_url,
                "original_content": original_content,
                "modified_content": modified_content,
                "diff_summary": diff_summary,
                "original_length": len(original_content),
                "modified_length": len(modified_content),
                "truncated": False  # JSON diffæ¨¡å¼ä¸ä¼šè¢«æˆªæ–­
            }
            
        except Exception as e:
            logger.error(f"ä¿®æ”¹æ–‡ä»¶å¤±è´¥ {minio_url}: {str(e)}")
            return {
                "file_path": minio_url,
                "original_content": original_content,
                "modified_content": original_content,  # ä¿æŒåŸæ ·
                "diff_summary": f"ä¿®æ”¹å¤±è´¥: {str(e)}",
                "error": str(e)
            }
    
    async def read_file_content(self, minio_url: str) -> str:
        """
        è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆä»MinIOï¼‰
        
        Args:
            minio_url: MinIOæ–‡ä»¶URLï¼ˆæ¥è‡ªRAG metadataï¼‰
            
        Returns:
            æ–‡ä»¶å†…å®¹
        """
        try:
            # ä»MinIOè¯»å–æ–‡ä»¶
            content = await self.kb_manager.read_file_from_minio(minio_url)
            
            if content:
                logger.info(f"æˆåŠŸä»MinIOè¯»å–æ–‡ä»¶, é•¿åº¦: {len(content)} å­—ç¬¦")
                return content
            else:
                logger.error(f"ä»MinIOè¯»å–æ–‡ä»¶å¤±è´¥: {minio_url}")
                return ""
                
        except Exception as e:
            logger.error(f"è¯»å–MinIOæ–‡ä»¶å¼‚å¸¸ {minio_url}: {str(e)}")
            return ""
    
    def _apply_diff_modifications(self, original_content: str, modifications: list) -> str:
        """
        å°†JSONæ ¼å¼çš„ä¿®æ”¹åº”ç”¨åˆ°åŸæ–‡æ¡£
        
        Args:
            original_content: åŸå§‹æ–‡æ¡£å†…å®¹
            modifications: [{"location": "...", "original_text": "...", "modified_text": "...", "reason": "..."}]
            
        Returns:
            ä¿®æ”¹åçš„æ–‡æ¡£å†…å®¹
        """
        result = original_content
        applied_count = 0
        failed_mods = []
        
        # æŒ‰é¡ºåºåº”ç”¨æ¯ä¸ªä¿®æ”¹
        for idx, mod in enumerate(modifications, 1):
            original_text = mod.get("original_text", "")
            modified_text = mod.get("modified_text", "")
            location = mod.get("location", "æœªæŒ‡å®šä½ç½®")
            reason = mod.get("reason", "")
            
            if not original_text:
                logger.warning(f"âš ï¸ ä¿®æ”¹ #{idx} [{location}]: ç¼ºå°‘original_text")
                failed_mods.append(f"{location} (ç¼ºå°‘åŸæ–‡)")
                continue
            
            if original_text in result:
                # æ›¿æ¢ç¬¬ä¸€æ¬¡å‡ºç°
                result = result.replace(original_text, modified_text, 1)
                applied_count += 1
                logger.info(f"âœ… ä¿®æ”¹ #{idx} [{location}]: {original_text[:40]}... â†’ {modified_text[:40]}...")
                if reason:
                    logger.info(f"   åŸå› : {reason}")
            else:
                logger.warning(f"âŒ ä¿®æ”¹ #{idx} [{location}]: æ— æ³•å®šä½")
                logger.warning(f"   æŸ¥æ‰¾æ–‡æœ¬: {original_text[:100]}...")
                failed_mods.append(location)
        
        logger.info(f"ğŸ“Š ä¿®æ”¹ç»Ÿè®¡: æˆåŠŸ {applied_count}/{len(modifications)}")
        if failed_mods:
            logger.warning(f"âš ï¸ æœªåº”ç”¨çš„ä¿®æ”¹: {', '.join(failed_mods)}")
        
        return result
    
    def _generate_diff_summary(self, original: str, modified: str) -> str:
        """ç”Ÿæˆç®€å•çš„diffæ‘˜è¦"""
        orig_lines = original.split('\n')
        mod_lines = modified.split('\n')
        
        added = len(mod_lines) - len(orig_lines)
        
        # ç®€å•ç»Ÿè®¡å˜åŒ–
        return f"è¡Œæ•°å˜åŒ–: {added:+d}ï¼ŒåŸ{len(orig_lines)}è¡Œ â†’ æ–°{len(mod_lines)}è¡Œ"

