"""æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥ä¸ä¿®æ”¹æ¨¡å— - åŸºäºå¤–éƒ¨RAGç³»ç»Ÿ"""
import json
import asyncio
from typing import List, Dict, Set
from pathlib import Path
from openai import AsyncOpenAI
import config
from rag_tool import RAGTool
from knowledge_base import KnowledgeBaseManager
from logger import get_logger

logger = get_logger(__name__)


class ConsistencyChecker:
    """æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥å™¨ - åˆ©ç”¨å¤–éƒ¨RAGç³»ç»Ÿ"""
    
    def __init__(self, api_key: str = None, base_url: str = None, model: str = None, project_id: str = None):
        self.model = model or config.MODEL_NAME
        self.client = AsyncOpenAI(
            api_key=api_key or config.OPENROUTER_API_KEY,
            base_url=base_url or config.OPENROUTER_BASE_URL
        )
        self.rag_tool = RAGTool()
        self.kb_manager = KnowledgeBaseManager()
        self.project_id = project_id  # ä¿å­˜é¡¹ç›®IDï¼Œç”¨äºReactAgent
    
    async def find_related_documents(self, 
                                     modification_point: str,
                                     project_id: str,
                                     current_file: str = None,
                                     top_k: int = 10) -> Dict:
        """
        æŸ¥æ‰¾ä¸ä¿®æ”¹ç‚¹ç›¸å…³çš„æ‰€æœ‰æ–‡æ¡£
        
        Args:
            modification_point: ä¿®æ”¹çš„å†…å®¹ç‚¹ï¼ˆå¦‚"æ—©å­£åˆ†ç±»"ï¼‰
            project_id: é¡¹ç›®ID
            current_file: å½“å‰æ­£åœ¨ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œç”¨äºæ’é™¤ï¼‰
            top_k: RAGå¬å›æ•°é‡
            
        Returns:
            {
                "related_files": {
                    "file_path1": [chunk1, chunk2, ...],
                    "file_path2": [chunk3, ...]
                },
                "total_files": int,
                "total_chunks": int
            }
        """
        logger.info(f"æŸ¥æ‰¾ä¸ '{modification_point}' ç›¸å…³çš„æ–‡æ¡£...")
        
        # è°ƒç”¨RAGæ£€ç´¢ï¼Œä½¿ç”¨metadata_filterç­›é€‰file_chunk
        search_result = await self.rag_tool.search(
            query=modification_point,
            project_id=project_id,
            top_k=top_k,
            use_refine=False,
            metadata_filter={"content_type": "file_chunk"}
        )
        
        if not search_result["success"] or not search_result["data"]:
            logger.warning("RAGæ£€ç´¢æœªè¿”å›ç»“æœ")
            return {
                "related_files": {},
                "total_files": 0,
                "total_chunks": 0
            }
        
        data = search_result["data"]
        
        # RAGè¿”å›çš„æ•°æ®å¯èƒ½æœ‰å¤šç§ç»“æ„ï¼Œéœ€è¦çµæ´»å¤„ç†
        all_chunks = []
        
        # æ–¹å¼1: ç›´æ¥çš„bundlesæ•°ç»„ï¼ˆæ¯ä¸ªbundleåŒ…å«conversations/factsï¼‰
        if data.get("bundles"):
            for bundle in data["bundles"]:
                # ä»conversationsä¸­æå–
                for conv in bundle.get("conversations", []):
                    all_chunks.append({
                        "content": conv.get("text", ""),
                        "score": conv.get("score", 1.0),
                        "metadata": conv.get("metadata", {})
                    })
                
                # ä»factsä¸­æå–ï¼ˆä¹Ÿå¯èƒ½åŒ…å«ç›¸å…³ä¿¡æ¯ï¼‰
                for fact in bundle.get("facts", []):
                    all_chunks.append({
                        "content": fact.get("content", ""),
                        "score": fact.get("score", 1.0),
                        "metadata": fact.get("metadata", {})
                    })
            
            if all_chunks:
                logger.info(f"ä»bundlesä¸­æå–åˆ° {len(all_chunks)} ä¸ªchunks")
        
        # æ–¹å¼2: short_term_memoryæ ¼å¼ï¼ˆæ—§ç‰ˆRAGï¼‰
        elif data.get("short_term_memory"):
            short_term_memory = data["short_term_memory"]
            
            # ä»conversationsæå–
            for conv in short_term_memory.get("conversations", []):
                all_chunks.append({
                    "content": conv.get("text", ""),
                    "score": 1.0,
                    "metadata": conv.get("metadata", {})
                })
            
            # ä»factsæå–
            for fact in short_term_memory.get("facts", []):
                all_chunks.append({
                    "content": fact.get("content", ""),
                    "score": 1.0,
                    "metadata": fact.get("metadata", {})
                })
            
            if all_chunks:
                logger.info(f"ä»short_term_memoryä¸­æå–åˆ° {len(all_chunks)} ä¸ªchunks")
        
        # ä½¿ç”¨æå–åˆ°çš„chunks
        bundles = all_chunks
        
        # æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„chunks
        related_files = {}
        for i, bundle in enumerate(bundles):
            # ä»bundleä¸­æå–æ–‡ä»¶è·¯å¾„å’Œå†…å®¹
            metadata = bundle.get("metadata", {})
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„å­—æ®µåï¼ˆä¸åŒRAGç‰ˆæœ¬å¯èƒ½ä½¿ç”¨ä¸åŒå­—æ®µï¼‰
            file_identifier = (
                metadata.get("file_path") or          # ä¼˜å…ˆä½¿ç”¨file_path
                metadata.get("source_identifier") or  # å…¶æ¬¡source_identifier
                metadata.get("minio_url") or          # ç„¶åminio_url
                metadata.get("source") or             # æœ€åsource
                "unknown"
            )
            
            # è°ƒè¯•ï¼šè¾“å‡ºå‰2ä¸ªchunkçš„metadata
            if i < 2:
                logger.info(f"Chunk {i} - å¯ç”¨å­—æ®µ: {list(metadata.keys())}")
                logger.info(f"Chunk {i} - æå–çš„file_identifier: {file_identifier}")
            
            # è·³è¿‡æ— æ•ˆURL
            if file_identifier == "unknown" or not file_identifier.startswith("http"):
                logger.warning(f"è·³è¿‡æ— æ•ˆçš„file_identifier: {file_identifier} (metadata keys: {list(metadata.keys())})")
                continue
            
            # è·³è¿‡å½“å‰æ­£åœ¨ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            if current_file and file_identifier == current_file:
                continue
            
            if file_identifier not in related_files:
                related_files[file_identifier] = []
            
            related_files[file_identifier].append({
                "content": bundle.get("content", ""),
                "score": bundle.get("score", 0),
                "metadata": metadata
            })
        
        if related_files:
            logger.info(f"æ–‡ä»¶æ ‡è¯†ç¬¦ç¤ºä¾‹: {list(related_files.keys())[:2]}")
        
        logger.info(f"æ‰¾åˆ° {len(related_files)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œå…± {len(bundles)} ä¸ªchunks")
        
        return {
            "related_files": related_files,
            "total_files": len(related_files),
            "total_chunks": len(bundles)
        }
    
    async def analyze_consistency(self,
                                  modification_request: str,
                                  current_file_content: str,
                                  related_files_content: Dict[str, str]) -> Dict:
        """
        åˆ†ææ–‡æ¡£é—´çš„ä¸€è‡´æ€§ï¼Œåˆ¤æ–­å“ªäº›æ–‡æ¡£éœ€è¦åŒæ­¥ä¿®æ”¹
        
        Args:
            modification_request: ç”¨æˆ·çš„ä¿®æ”¹è¦æ±‚
            current_file_content: å½“å‰æ–‡ä»¶å†…å®¹ï¼ˆå¯é€‰ï¼Œå¯èƒ½ä¸ºNoneï¼‰
            related_files_content: {file_path: file_content}
            
        Returns:
            {
                "needs_modification": [file_path1, file_path2, ...],
                "modification_type": str,
                "consistency_analysis": str,
                "global_consistency_required": bool
            }
        """
        # å¦‚æœæ²¡æœ‰å…¶ä»–æ–‡ä»¶ï¼Œç›´æ¥è¿”å›éœ€è¦ä¿®æ”¹
        if not related_files_content:
            return {
                "needs_modification": [],
                "modification_type": "æ–‡æ¡£ä¿®æ”¹",
                "consistency_analysis": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
                "global_consistency_required": False
            }
        
        # æ„å»ºåˆ†æprompt
        files_summary = []
        for file_path, content in list(related_files_content.items())[:5]:  # æœ€å¤šåˆ†æ5ä¸ªæ–‡ä»¶
            # ä»è·¯å¾„æå–æ–‡ä»¶å
            file_name = file_path.split('/')[-1] if '/' in file_path else file_path.split('\\')[-1]
            files_summary.append(
                f"æ–‡ä»¶: {file_name}\n"
                f"å†…å®¹é¢„è§ˆ: {content[:300]}...\n"
            )
        
        # å¦‚æœæä¾›äº†å½“å‰æ–‡ä»¶å†…å®¹ï¼ŒåŒ…å«åœ¨åˆ†æä¸­
        current_file_section = ""
        if current_file_content:
            current_file_section = f"""
å½“å‰æ–‡ä»¶å†…å®¹é¢„è§ˆ:
{current_file_content[:500]}...
"""
        
        analysis_prompt = f"""ä½ éœ€è¦åˆ†æä»¥ä¸‹ä¿®æ”¹éœ€æ±‚å¯¹æ–‡æ¡£çš„å½±å“ã€‚

ä¿®æ”¹è¦æ±‚:
{modification_request}
{current_file_section}
ç›¸å…³æ–‡æ¡£:
{chr(10).join(files_summary)}

è¯·åˆ†æ:
1. æ ¹æ®ä¿®æ”¹è¦æ±‚ï¼Œå“ªäº›æ–‡æ¡£éœ€è¦ä¿®æ”¹ï¼Ÿ
2. ä¿®æ”¹ç±»å‹æ˜¯ä»€ä¹ˆï¼ˆæœ¯è¯­ç»Ÿä¸€/æ•°æ®æ›´æ–°/æ–¹æ³•æ”¹è¿›ç­‰ï¼‰ï¼Ÿ
3. ä¸ºä»€ä¹ˆè¿™äº›æ–‡æ¡£éœ€è¦ä¿®æ”¹ï¼Ÿ

ä»¥JSONæ ¼å¼è¿”å›:
{{
  "needs_modification": ["file1.md", "file2.md"],  // éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨
  "modification_type": "æœ¯è¯­ç»Ÿä¸€/è§‚ç‚¹è°ƒæ•´/æ•°æ®æ›´æ–°ç­‰",
  "consistency_analysis": "è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›æ–‡æ¡£éœ€è¦ä¿®æ”¹",
  "global_consistency_required": true/false
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            logger.info("åˆ†ææ–‡æ¡£ä¸€è‡´æ€§...")
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ä¸€è‡´æ€§åˆ†æå¸ˆã€‚"},
                    {"role": "user", "content": analysis_prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            
            content = response.choices[0].message.content.strip()
            
            # æå–JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            analysis = json.loads(content)
            logger.info(f"ä¸€è‡´æ€§åˆ†æå®Œæˆ: {json.dumps(analysis, ensure_ascii=False)}")
            return analysis
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
            # é»˜è®¤æ‰€æœ‰ç›¸å…³æ–‡ä»¶éƒ½éœ€è¦ä¿®æ”¹
            return {
                "needs_modification": list(related_files_content.keys()),
                "modification_type": "ä¸€è‡´æ€§ä¿®æ”¹",
                "consistency_analysis": "é»˜è®¤å…¨éƒ¨åŒæ­¥ä¿®æ”¹",
                "global_consistency_required": True
            }
    
    async def generate_modifications(self,
                                    modification_request: str,
                                    current_modification: str,
                                    files_to_modify: Dict[str, str],
                                    project_id: str = None) -> List[Dict]:
        """
        ä¸ºéœ€è¦ä¿®æ”¹çš„æ–‡æ¡£ç”Ÿæˆä¿®æ”¹ç‰ˆæœ¬
        
        Args:
            modification_request: ä¿®æ”¹è¦æ±‚
            current_modification: å½“å‰æ–‡ä»¶çš„ä¿®æ”¹ç¤ºä¾‹
            files_to_modify: {file_path: file_content}
            
        Returns:
            [
                {
                    "file_path": str,
                    "original_content": str,
                    "modified_content": str,
                    "diff_summary": str
                }
            ]
        """
        logger.info(f"ğŸš€ å¹¶è¡Œå¤„ç† {len(files_to_modify)} ä¸ªæ–‡æ¡£çš„ä¿®æ”¹...")
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡æ¡£
        tasks = [
            self._modify_single_file(
                modification_request,
                current_modification,
                file_path,
                original_content,
                project_id=project_id or self.project_id
            )
            for file_path, original_content in files_to_modify.items()
        ]
        
        modifications = await asyncio.gather(*tasks)
        
        logger.info(f"âœ… {len(modifications)} ä¸ªæ–‡æ¡£å¤„ç†å®Œæˆ")
        return list(modifications)
    
    async def _modify_single_file(self,
                                  modification_request: str,
                                  current_modification: str,
                                  minio_url: str,
                                  original_content: str,
                                  project_id: str = None) -> Dict:
        """
        ä¿®æ”¹å•ä¸ªæ–‡ä»¶ - æ–°æµç¨‹ï¼šAIè¯„ä¼° â†’ ReactAgentç”Ÿæˆ â†’ Diff
        
        æµç¨‹ï¼š
        1. AIè¯„ä¼°éœ€è¦ä¿®æ”¹çš„ç‚¹ï¼ˆä¸ç”Ÿæˆå…·ä½“ä¿®æ”¹å†…å®¹ï¼‰
        2. å°†è¯„ä¼°ç»“æœå’ŒåŸæ–‡ä¼ ç»™ReactAgentç”Ÿæˆä¿®æ”¹åçš„å†…å®¹
        3. ç”Ÿæˆdiff
        """
        # ä»URLæå–æ–‡ä»¶å
        file_name = minio_url.split('/')[-1] if '/' in minio_url else minio_url
        
        try:
            # ========== ç¬¬1æ­¥ï¼šAIè¯„ä¼°éœ€è¦ä¿®æ”¹çš„ç‚¹ ==========
            logger.info(f"ğŸ” ç¬¬1æ­¥ï¼šAIè¯„ä¼°æ–‡æ¡£ä¿®æ”¹ç‚¹: {file_name}")
            evaluation = await self._evaluate_modification_points(
                modification_request,
                current_modification,
                file_name,
                original_content
            )
            
            if not evaluation.get("needs_modification", True):
                logger.info(f"â„¹ï¸ AIè®¤ä¸ºæ–‡æ¡£ {file_name} æ— éœ€ä¿®æ”¹")
                return {
                    "file_path": minio_url,
                    "original_content": original_content,
                    "modified_content": original_content,
                    "diff_summary": "æ— éœ€ä¿®æ”¹",
                    "original_length": len(original_content),
                    "modified_length": len(original_content),
                    "evaluation": evaluation,
                    "react_thinking_process": [],
                    "react_search_history": [],
                    "truncated": False
                }
            
            # ========== ç¬¬2æ­¥ï¼šè°ƒç”¨ReactAgentç”Ÿæˆä¿®æ”¹åçš„å†…å®¹ ==========
            logger.info(f"ğŸ¤– ç¬¬2æ­¥ï¼šè°ƒç”¨ReactAgentç”Ÿæˆä¿®æ”¹åçš„å†…å®¹")
            react_result = await self._generate_with_react_agent(
                modification_request,
                original_content,
                evaluation,
                project_id=project_id or self.project_id
            )
            
            modified_content = react_result.get("content", original_content)
            thinking_process = react_result.get("thinking_process", [])
            search_history = react_result.get("search_history", [])
            
            # ========== ç¬¬3æ­¥ï¼šç”ŸæˆDiff ==========
            logger.info(f"ğŸ“Š ç¬¬3æ­¥ï¼šç”Ÿæˆdiff")
            diff_summary = f"âœ… ReactAgentå·²ç”Ÿæˆä¿®æ”¹å†…å®¹"
            
            return {
                "file_path": minio_url,
                "original_content": original_content,
                "modified_content": modified_content,
                "diff_summary": diff_summary,
                "original_length": len(original_content),
                "modified_length": len(modified_content),
                "evaluation": evaluation,
                "react_thinking_process": thinking_process,  # ReactAgentæ€è€ƒè¿‡ç¨‹
                "react_search_history": search_history,  # ReactAgentæœç´¢å†å²
                "truncated": False
            }
            
        except Exception as e:
            logger.error(f"ä¿®æ”¹æ–‡ä»¶å¤±è´¥ {minio_url}: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "file_path": minio_url,
                "original_content": original_content,
                "modified_content": original_content,
                "diff_summary": f"ä¿®æ”¹å¤±è´¥: {str(e)}",
                "original_length": len(original_content),
                "modified_length": len(original_content),
                "evaluation": {},
                "react_thinking_process": [],
                "react_search_history": [],
                "truncated": False
            }
    
    async def _evaluate_modification_points(self,
                                           modification_request: str,
                                           current_modification: str,
                                           file_name: str,
                                           original_content: str) -> Dict:
        """
        AIè¯„ä¼°é˜¶æ®µï¼šåªè¯„ä¼°éœ€è¦ä¿®æ”¹çš„ç‚¹ï¼Œä¸ç”Ÿæˆå…·ä½“ä¿®æ”¹å†…å®¹
        
        Returns:
            {
                "needs_modification": True/False,
                "modification_points": [
                    {
                        "location": "ç« èŠ‚åç§°æˆ–ä½ç½®",
                        "original_text": "éœ€è¦ä¿®æ”¹çš„åŸå§‹æ–‡æœ¬ç‰‡æ®µ",
                        "modification_reason": "ä¸ºä»€ä¹ˆéœ€è¦ä¿®æ”¹",
                        "modification_type": "ä¿®æ”¹ç±»å‹ï¼ˆå¦‚æœ¯è¯­ç»Ÿä¸€ã€å†…å®¹è¡¥å……ç­‰ï¼‰"
                    }
                ],
                "overall_guidance": "æ•´ä½“ä¿®æ”¹æŒ‡å¯¼"
            }
        """
        # æ„å»ºè¯„ä¼°prompt
        reference_section = ""
        if current_modification:
            reference_section = f"""
å‚è€ƒä¿®æ”¹ç¤ºä¾‹ï¼ˆä¿æŒä¸€è‡´çš„ä¿®æ”¹é£æ ¼ï¼‰:
{current_modification[:500]}...
"""
        
        evaluation_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£è¯„ä¼°ä¸“å®¶ã€‚è¯·**æ·±å…¥åˆ†æ**ä»¥ä¸‹æ–‡æ¡£ï¼Œè¯„ä¼°éœ€è¦ä¿®æ”¹çš„ç‚¹ã€‚

ä¿®æ”¹è¦æ±‚:
{modification_request}
{reference_section}
å¾…è¯„ä¼°æ–‡ä»¶: {file_name}
æ–‡ä»¶å†…å®¹:
{original_content}

ä½ çš„ä»»åŠ¡æ˜¯**æ·±åº¦è¯„ä¼°å¹¶æå–**éœ€è¦ä¿®æ”¹çš„ä½ç½®ï¼š

**ç¬¬ä¸€æ­¥ï¼šæ·±åº¦åˆ†ææ–‡æ¡£**
- ä»”ç»†é˜…è¯»æ–‡æ¡£ï¼Œç†è§£å…¶ç»“æ„ã€å†…å®¹å’Œé€»è¾‘
- æ ¹æ®ä¿®æ”¹è¦æ±‚ï¼Œè¯†åˆ«å“ªäº›éƒ¨åˆ†**çœŸæ­£éœ€è¦ä¿®æ”¹**
- ä¸è¦åªçœ‹å…³é”®è¯ï¼Œè¦ç†è§£**ä¿®æ”¹çš„ç›®çš„å’Œæ„ä¹‰**

**ç¬¬äºŒæ­¥ï¼šè¯„ä¼°ä¿®æ”¹ç‚¹**
1. **è¯†åˆ«ä¿®æ”¹ç‚¹**: æ‰¾å‡ºæ–‡æ¡£ä¸­å“ªäº›ç« èŠ‚/æ®µè½éœ€è¦ä¿®æ”¹
   - å¯¹äº"æœ¯è¯­æ›¿æ¢"ï¼šä¸æ˜¯ç®€å•åœ°æ‰¾åŒ…å«æœ¯è¯­çš„ç« èŠ‚ï¼Œè€Œæ˜¯åˆ†ææ›¿æ¢åå¯¹å†…å®¹çš„å½±å“
   - å¯¹äº"å†…å®¹è¡¥å……"ï¼šåˆ†æå“ªäº›éƒ¨åˆ†å†…å®¹ä¸è¶³ï¼Œéœ€è¦è¡¥å……ä»€ä¹ˆ
   - å¯¹äº"è§‚ç‚¹è°ƒæ•´"ï¼šåˆ†æå“ªäº›è§‚ç‚¹ä¸è¦æ±‚ä¸ç¬¦ï¼Œå¦‚ä½•è°ƒæ•´

2. **ç²¾ç¡®æå–åŸæ–‡**: ä»æ–‡æ¡£ä¸­æå–éœ€è¦ä¿®æ”¹çš„åŸæ–‡ç‰‡æ®µï¼ˆç”¨äºå®šä½ï¼‰
   
3. **æ·±åº¦è¯´æ˜åŸå› **: ä¸è¦åªè¯´"åŒ…å«XXæœ¯è¯­"ï¼Œè€Œè¦è¯´æ˜ï¼š
   - è¿™éƒ¨åˆ†ä¸ºä»€ä¹ˆéœ€è¦ä¿®æ”¹ï¼Ÿ
   - ä¿®æ”¹åä¼šæœ‰ä»€ä¹ˆæ•ˆæœï¼Ÿ
   - è¿™ä¸ªä¿®æ”¹å¯¹æ•´ä½“æ–‡æ¡£çš„ä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ
   
4. **åˆ†ç±»ä¿®æ”¹**: è¯´æ˜ä¿®æ”¹ç±»å‹ï¼ˆå¦‚æœ¯è¯­ç»Ÿä¸€ã€å†…å®¹è¡¥å……ã€è§‚ç‚¹è°ƒæ•´ã€é€»è¾‘ä¼˜åŒ–ç­‰ï¼‰

**è¾“å‡ºæ ¼å¼**: ä½¿ç”¨ä»¥ä¸‹JSONæ ¼å¼ï¼š
```json
{{
  "needs_modification": true/false,
  "modification_points": [
    {{
      "location": "æ¸…æ™°çš„ä½ç½®æè¿°ï¼ˆå¦‚'ç¬¬1ç«  Introduction ç¬¬ä¸€æ®µ'ï¼‰",
      "original_text": "ä»æ–‡æ¡£ä¸­é€å­—ç²¾ç¡®å¤åˆ¶çš„åŸæ–‡ç‰‡æ®µï¼ˆå®Œæ•´çš„æ®µè½æˆ–å¥å­ï¼Œç”¨äºç²¾ç¡®å®šä½ï¼‰",
      "modification_reason": "ä¸ºä»€ä¹ˆéœ€è¦ä¿®æ”¹è¿™éƒ¨åˆ†",
      "modification_type": "ä¿®æ”¹ç±»å‹",
      "is_full_chapter": true/false
    }}
  ],
  "overall_guidance": "æ•´ä½“ä¿®æ”¹æŒ‡å¯¼è¯´æ˜"
}}
```

**CRITICAL RULES - æŒ‰ç« èŠ‚æå–ï¼Œä¸¥ç¦é‡å¤**:

**æ ¸å¿ƒç­–ç•¥ï¼šåªæå–Markdownæ ‡é¢˜ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ‰©å±•åˆ°å®Œæ•´ç« èŠ‚**

1. **original_textæå–è§„åˆ™**ï¼š
   - âœ… **åªæå–Markdownæ ‡é¢˜è¡Œ**ï¼ˆå¦‚ "# 4 Memory Modeling in MemOS"ï¼‰
   - âœ… ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰©å±•åˆ°è¯¥æ ‡é¢˜å¯¹åº”çš„å®Œæ•´ç« èŠ‚å†…å®¹
   - âœ… æ”¯æŒæ‰€æœ‰çº§åˆ«çš„æ ‡é¢˜ï¼š#, ##, ###, ####ç­‰
   - âŒ ä¸è¦æå–å®Œæ•´å†…å®¹ï¼ˆå¤ªé•¿ï¼Œå®¹æ˜“è¶…tokené™åˆ¶ï¼‰
   - âŒ ä¸è¦ä½¿ç”¨çœç•¥å·

2. **å±‚çº§äº’æ–¥åŸåˆ™ï¼ˆé‡è¦ï¼é˜²æ­¢é‡å¤ï¼‰**ï¼š
   - âŒ **ç¦æ­¢åŒæ—¶æå–çˆ¶ç« èŠ‚å’Œå­ç« èŠ‚**
   - ä¾‹å¦‚ï¼šå¦‚æœæå–äº† `# 3 Design Philosophy`ï¼ˆçˆ¶ç« èŠ‚ï¼‰ï¼Œå°±**ä¸è¦**å†æå– `## 3.1 Vision`ã€`## 3.2 From OS`ï¼ˆå­ç« èŠ‚ï¼‰
   - åŸå› ï¼šçˆ¶ç« èŠ‚åŒ…å«äº†æ‰€æœ‰å­ç« èŠ‚çš„å†…å®¹ï¼Œé‡å¤æå–ä¼šå¯¼è‡´å†…å®¹é‡å¤
   
3. **ä¼˜å…ˆçº§é€‰æ‹©**ï¼š
   - ä¼˜å…ˆé€‰æ‹©**é¡¶å±‚ç« èŠ‚**ï¼ˆ#ï¼‰ï¼šå¦‚æœæ•´ç« éƒ½éœ€è¦ä¿®æ”¹ï¼Œåªæå–é¡¶å±‚æ ‡é¢˜
   - ä»…åœ¨**éƒ¨åˆ†å­ç« èŠ‚**éœ€è¦ä¿®æ”¹æ—¶ï¼Œæ‰æå–å­ç« èŠ‚ï¼ˆ##ï¼‰
   - ç¤ºä¾‹ï¼š
     * å¦‚æœç¬¬3ç« çš„3.1ã€3.2éƒ½éœ€è¦ä¿®æ”¹ â†’ åªæå– `# 3 ç« èŠ‚å`
     * å¦‚æœåªæœ‰3.2éœ€è¦ä¿®æ”¹ï¼Œ3.1ä¸éœ€è¦ â†’ åªæå– `## 3.2 å°èŠ‚å`

4. **ä¿®æ”¹ç²’åº¦å»ºè®®**ï¼š
   - å¯¹äº"æœ¯è¯­ç»Ÿä¸€"ç­‰å…¨æ–‡ä¿®æ”¹ï¼šæŒ‰**é¡¶å±‚ç« èŠ‚**ï¼ˆ#ï¼‰æå–ï¼Œä¸€ç« ä¸€ä¸ªä¿®æ”¹ç‚¹
   - å¯¹äº"å±€éƒ¨ä¿®æ”¹"ï¼šæŒ‰éœ€è¦ä¿®æ”¹çš„**æœ€å°ç« èŠ‚å•ä½**æå–

5. **æ·±åº¦åˆ†æè¦æ±‚ï¼ˆé‡è¦ï¼ï¼‰**ï¼š
   - âŒ **ç¦æ­¢**ç®€å•åœ°è¯´"åŒ…å«XXæœ¯è¯­"ã€"éœ€è¦æ›¿æ¢XX"è¿™ç§æµ…å±‚åŸå› 
   - âœ… **å¿…é¡»**æ·±å…¥åˆ†æï¼š
     * è¿™ä¸ªç« èŠ‚çš„æ ¸å¿ƒå†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ
     * ä¸ºä»€ä¹ˆè¿™éƒ¨åˆ†éœ€è¦ä¿®æ”¹ï¼Ÿï¼ˆä¸æ˜¯"å› ä¸ºæœ‰å…³é”®è¯"ï¼Œè€Œæ˜¯"è¿™éƒ¨åˆ†è®²äº†ä»€ä¹ˆï¼Œä¿®æ”¹åæœ‰ä»€ä¹ˆæ„ä¹‰"ï¼‰
     * ä¿®æ”¹åå¯¹è¯»è€…ç†è§£æœ‰ä»€ä¹ˆå¸®åŠ©ï¼Ÿ
     * è¿™ä¸ªä¿®æ”¹åœ¨æ•´ä¸ªæ–‡æ¡£ä¸­çš„ä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ
   - âœ… **modification_reasonè‡³å°‘è¦åŒ…å«**ï¼š
     * ç« èŠ‚çš„ä¸»è¦å†…å®¹æ¦‚è¿°
     * ä¿®æ”¹çš„å…·ä½“åŸå› å’Œç›®çš„
     * ä¿®æ”¹åçš„é¢„æœŸæ•ˆæœ
   - ç¤ºä¾‹å¯¹æ¯”ï¼š
     * âŒ å·®ï¼š"ç« èŠ‚åŒ…å«'MemOS'æœ¯è¯­ï¼Œéœ€è¦æ›¿æ¢"
     * âœ… å¥½ï¼š"æœ¬ç« ä»‹ç»äº†ç³»ç»Ÿæ¶æ„çš„ä¸‰å±‚è®¾è®¡ï¼ŒåŒ…æ‹¬æ¥å£å±‚ã€æ“ä½œå±‚å’ŒåŸºç¡€è®¾æ–½å±‚ã€‚æ ‡é¢˜å’Œæ­£æ–‡å¤šå¤„ä½¿ç”¨'MemOS'æœ¯è¯­ï¼Œéœ€è¦ç»Ÿä¸€æ›¿æ¢ä¸º'mem0'ä»¥ä¿æŒå“ç‰Œä¸€è‡´æ€§ã€‚è¿™ç§æ›¿æ¢ä¸ä»…æ˜¯æ–‡å­—å˜æ›´ï¼Œæ›´ä½“ç°äº†ç³»ç»Ÿä»æ¦‚å¿µåˆ°äº§å“çš„æ¼”è¿›ï¼ŒReactAgentéœ€è¦åœ¨ä¿æŒæŠ€æœ¯æ·±åº¦çš„åŒæ—¶ï¼Œç¡®ä¿æ–°æœ¯è¯­è‡ªç„¶èå…¥æ¶æ„è¯´æ˜ä¸­ã€‚"

**æ­£ç¡®ç¤ºä¾‹** âœ…ï¼ˆæ·±åº¦åˆ†æï¼‰:
```json
{{
  "modification_points": [
    {{
      "location": "ç¬¬3ç«  Design Philosophy",
      "original_text": "# 3 MemOS Design Philosophy",
      "modification_reason": "æœ¬ç« è¯¦ç»†é˜è¿°äº†MemOSçš„è®¾è®¡ç†å¿µï¼ŒåŒ…æ‹¬å°†è®°å¿†è§†ä¸ºç³»ç»Ÿèµ„æºã€æ¼”åŒ–ä½œä¸ºæ ¸å¿ƒèƒ½åŠ›ç­‰æ ¸å¿ƒæ€æƒ³ã€‚ç« èŠ‚æ ‡é¢˜å’Œæ­£æ–‡å¤šå¤„ä½¿ç”¨'MemOS'æœ¯è¯­ï¼Œéœ€è¦ç»Ÿä¸€æ›¿æ¢ä¸º'mem0'ä»¥ä¿æŒå“ç‰Œä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œè¿™ç§æ›¿æ¢ä¸ä»…æ˜¯ç®€å•çš„æ–‡å­—å˜æ›´ï¼Œæ›´ä½“ç°äº†ä»'Memory OS'åˆ°'mem0'çš„å“ç‰Œå‡çº§ï¼ŒReactAgentéœ€è¦åœ¨ä¿æŒåŸæœ‰æŠ€æœ¯æ·±åº¦çš„åŸºç¡€ä¸Šï¼Œç¡®ä¿æ–°æœ¯è¯­çš„è‡ªç„¶èå…¥ã€‚",
      "modification_type": "æœ¯è¯­ç»Ÿä¸€ä¸å“ç‰Œå‡çº§",
      "is_full_chapter": true
    }},
    {{
      "location": "ç¬¬5.2èŠ‚ Execution Path",
      "original_text": "## 5.2 Execution Path and Interaction Flow of MemOS",
      "modification_reason": "æœ¬èŠ‚é€šè¿‡å…·ä½“çš„æ‰§è¡Œæµç¨‹æ¡ˆä¾‹ï¼ˆ'æŸ¥è¯¢å»å¹´çš„åŒ»ç–—è®°å½•'ï¼‰å±•ç¤ºäº†MemOSå„æ¨¡å—çš„ååŒå·¥ä½œæœºåˆ¶ã€‚è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„æŠ€æœ¯è¯´æ˜ç« èŠ‚ï¼Œä¸ä»…æ ‡é¢˜åŒ…å«æœ¯è¯­ï¼Œæ­£æ–‡ä¸­çš„æ¶æ„æè¿°ã€æ¨¡å—äº¤äº’è¯´æ˜ä¹Ÿå¤§é‡ä½¿ç”¨äº†'MemOS'ã€‚éœ€è¦ReactAgentåœ¨æ›¿æ¢æœ¯è¯­çš„åŒæ—¶ï¼Œç¡®ä¿æŠ€æœ¯æè¿°çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼Œç‰¹åˆ«æ˜¯è¦ä¿æŒç¤ºä¾‹çš„è¿è´¯æ€§å’Œå¯ç†è§£æ€§ã€‚",
      "modification_type": "æœ¯è¯­ç»Ÿä¸€ + æŠ€æœ¯æè¿°ä¼˜åŒ–",
      "is_full_chapter": true
    }}
  ]
}}
```

**é”™è¯¯ç¤ºä¾‹** âŒï¼ˆé‡å¤æå–çˆ¶å­ç« èŠ‚ï¼‰:
```json
{{
  "modification_points": [
    {{
      "original_text": "# 3 MemOS Design Philosophy",  // âŒ æå–äº†çˆ¶ç« èŠ‚
      "is_full_chapter": true
    }},
    {{
      "original_text": "## 3.1 Vision of MemOS",  // âŒ åˆæå–äº†å­ç« èŠ‚ï¼Œä¼šå¯¼è‡´é‡å¤ï¼
      "is_full_chapter": true
    }},
    {{
      "original_text": "## 3.2 From Computer OS",  // âŒ åˆæå–äº†å¦ä¸€ä¸ªå­ç« èŠ‚ï¼Œä¼šå¯¼è‡´é‡å¤ï¼
      "is_full_chapter": true
    }}
  ]
}}
```

**æ­£ç¡®åšæ³•**ï¼šä¸Šé¢çš„æƒ…å†µåªéœ€è¦æå– `# 3 MemOS Design Philosophy` å³å¯ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åŒ…å«3.1ã€3.2ç­‰æ‰€æœ‰å­ç« èŠ‚ã€‚

**éƒ¨åˆ†ä¿®æ”¹ç¤ºä¾‹** âœ…ï¼ˆåªä¿®æ”¹æŸä¸ªå­ç« èŠ‚ï¼‰:
```json
{{
  "modification_points": [
    {{
      "location": "4.2 Memory Cube",
      "original_text": "## 4.2 Memory Cube as Core Resource",
      "modification_reason": "åªæœ‰4.2éœ€è¦ä¿®æ”¹ï¼Œ4.1ä¸éœ€è¦",
      "modification_type": "å†…å®¹è¡¥å……",
      "is_full_chapter": true
    }}
  ]
}}
```

**æ®µè½çº§ä¿®æ”¹** âœ…ï¼ˆå¦‚æœç¡®å®åªéœ€è¦æ”¹ä¸€æ®µï¼‰:
```json
{{
  "location": "Introduction ç¬¬2æ®µ",
  "original_text": "MemOS is a revolutionary memory management system designed for large language models.",
  "modification_reason": "è¯¥æ®µè½åŒ…å«'MemOS'æœ¯è¯­",
  "modification_type": "æœ¯è¯­ç»Ÿä¸€",
  "is_full_chapter": false
}}
```

**å·¥ä½œåŸç†**ï¼š
1. ä½ åªéœ€æå–æ ‡é¢˜ï¼š`"# 4 Memory Modeling in MemOS"`
2. ç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹è¿™æ˜¯æ ‡é¢˜ï¼ˆä»¥#å¼€å¤´ï¼‰
3. ç³»ç»Ÿè‡ªåŠ¨æ‰©å±•åˆ°å®Œæ•´ç« èŠ‚ï¼ˆä»è¿™ä¸ªæ ‡é¢˜åˆ°ä¸‹ä¸€ä¸ªåŒçº§æ ‡é¢˜ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼‰
4. ReactAgentåŸºäºå®Œæ•´ç« èŠ‚ç”Ÿæˆä¿®æ”¹åçš„å®Œæ•´ç« èŠ‚
5. å®Œæ•´æ›¿æ¢ï¼Œä¸ä¼šä¸¢å¤±å†…å®¹

**å»ºè®®**ï¼š
- ä¼˜å…ˆä½¿ç”¨æ ‡é¢˜æå–æ–¹å¼ï¼ˆæœ€ç®€å•ã€æœ€å¯é ï¼‰
- ä¸€ä¸ªç« èŠ‚ä¸€ä¸ªmodification_point
- è®©ç³»ç»Ÿè‡ªåŠ¨å¤„ç†ç« èŠ‚è¾¹ç•Œ

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚å¦‚æœæ— æ³•è¿”å›JSONï¼Œè¯·è¿”å›ï¼š{{"needs_modification": false, "modification_points": [], "overall_guidance": "æ— æ³•åˆ†æ"}}"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£è¯„ä¼°ä¸“å®¶ã€‚ä½ éœ€è¦æ·±å…¥åˆ†ææ–‡æ¡£å†…å®¹ï¼Œç†è§£ä¿®æ”¹çš„æ·±å±‚ç›®çš„ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æœç´¢å…³é”®è¯ã€‚å¯¹äºç« èŠ‚ä¿®æ”¹ï¼Œæå–Markdownæ ‡é¢˜å³å¯ï¼ˆå¦‚'# 4 ç« èŠ‚å'ï¼‰ï¼Œä½†modification_reasonå¿…é¡»ä½“ç°ä½ çš„æ·±åº¦æ€è€ƒå’Œåˆ†æã€‚"},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.3,  # æé«˜æ¸©åº¦ï¼Œè®©AIæ›´æœ‰åˆ›é€ æ€§åˆ†æ
                max_tokens=8000
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # æå–JSON
            if "```json" in raw_response:
                json_str = raw_response.split("```json")[1].split("```")[0].strip()
            elif "```" in raw_response:
                json_str = raw_response.split("```")[1].split("```")[0].strip()
            else:
                json_str = raw_response
            
            evaluation = json.loads(json_str)
            
            modification_points = evaluation.get("modification_points", [])
            logger.info(f"ğŸ“‹ è¯„ä¼°ç»“æœ: éœ€è¦ä¿®æ”¹ {len(modification_points)} å¤„")
            
            # ğŸ”§ ç¬¬1æ­¥ï¼šå»é™¤é‡å¤çš„çˆ¶å­ç« èŠ‚ï¼ˆé˜²æ­¢AIè¿åè§„åˆ™ï¼‰
            modification_points = self._deduplicate_hierarchical_chapters(modification_points, original_content)
            logger.info(f"ğŸ”„ å»é‡å: {len(modification_points)} å¤„")
            
            # ğŸ”§ ç¬¬2æ­¥ï¼šç«‹å³æ‰©å±•æ‰€æœ‰ä¿®æ”¹ç‚¹çš„original_text
            for idx, point in enumerate(modification_points, 1):
                location = point.get('location', 'æœªçŸ¥ä½ç½®')
                mod_type = point.get('modification_type', 'æœªçŸ¥ç±»å‹')
                original_text = point.get('original_text', '')
                is_full_chapter = point.get('is_full_chapter', False)
                
                logger.info(f"  {idx}. [{location}] {mod_type} (åŸæ–‡: {len(original_text)}å­—ç¬¦)")
                
                # å¦‚æœæ˜¯æ ‡é¢˜ï¼ˆis_full_chapter=trueï¼‰ï¼Œè‡ªåŠ¨æ‰©å±•åˆ°å®Œæ•´ç« èŠ‚
                if is_full_chapter and original_text.strip().startswith('#'):
                    logger.info(f"     ğŸ” æ£€æµ‹åˆ°ç« èŠ‚æ ‡é¢˜ï¼Œæ‰©å±•åˆ°å®Œæ•´ç« èŠ‚...")
                    expanded = self._expand_original_text(original_content, original_text)
                    if len(expanded) > len(original_text):
                        point['original_text'] = expanded
                        logger.info(f"     âœ… æ‰©å±•: {len(original_text)} â†’ {len(expanded)} å­—ç¬¦")
                    else:
                        logger.warning(f"     âš ï¸ æ‰©å±•å¤±è´¥ï¼Œä¿æŒåŸæ ·")
            
            # æ›´æ–°evaluationä¸­çš„modification_points
            evaluation['modification_points'] = modification_points
            
            return evaluation
            
        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
            logger.error(f"AIåŸå§‹å“åº”: {raw_response[:500] if 'raw_response' in locals() else 'N/A'}")
            return {
                "needs_modification": False,
                "modification_points": [],
                "overall_guidance": f"è¯„ä¼°å¤±è´¥: {str(e)}"
            }
    
    async def _generate_with_react_agent(self,
                                        modification_request: str,
                                        original_content: str,
                                        evaluation: Dict,
                                        project_id: str = None) -> Dict:
        """
        æ™ºèƒ½é€‰æ‹©ä¿®æ”¹ç­–ç•¥ï¼š
        - ç®€å•æœ¯è¯­æ›¿æ¢ â†’ ç›´æ¥å­—ç¬¦ä¸²æ›¿æ¢
        - å¤æ‚ä¿®æ”¹ â†’ ReactAgentæœç´¢+ç”Ÿæˆ
        
        Args:
            modification_request: ä¿®æ”¹è¦æ±‚
            original_content: åŸå§‹æ–‡æ¡£å†…å®¹
            evaluation: AIè¯„ä¼°ç»“æœï¼ˆåŒ…å«modification_pointsï¼‰
            project_id: é¡¹ç›®ID
            
        Returns:
            {
                "content": ä¿®æ”¹åçš„æ–‡æ¡£å†…å®¹,
                "thinking_process": ReactAgentæ€è€ƒè¿‡ç¨‹,
                "search_history": ReactAgentæœç´¢å†å²
            }
        """
        # å¯¼å…¥ReactAgentï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
        from react_agent import ReactAgent
        
        modification_points = evaluation.get("modification_points", [])
        
        # å¦‚æœæ²¡æœ‰å…·ä½“çš„ä¿®æ”¹ç‚¹ï¼Œç›´æ¥è¿”å›åŸæ–‡
        if not modification_points:
            logger.warning("è¯„ä¼°ç»“æœä¸­æ²¡æœ‰å…·ä½“ä¿®æ”¹ç‚¹ï¼Œè¿”å›åŸæ–‡")
            return {
                "content": original_content,
                "thinking_process": [],
                "search_history": []
            }
        
        # åˆ›å»ºReactAgentå®ä¾‹
        # max_iterations=5: å…è®¸å¤šæ¬¡searchæœé›†èµ„æ–™ï¼Œä½†generateåªèƒ½1æ¬¡
        agent = ReactAgent(
            max_iterations=5,  # âœ… å…è®¸å¤šæ¬¡æœç´¢èµ„æ–™
            project_id=project_id or self.project_id,
            top_k=10,
            use_refine=False
        )
        
        # ğŸš€ å¹¶è¡Œå¤„ç†æ‰€æœ‰ä¿®æ”¹ç‚¹
        logger.info(f"ğŸš€ å¹¶è¡Œå¤„ç† {len(modification_points)} ä¸ªä¿®æ”¹ç‚¹...")
        
        async def process_single_point(idx, point):
            """å¤„ç†å•ä¸ªä¿®æ”¹ç‚¹"""
            try:
                location = point.get("location", "æœªçŸ¥ä½ç½®")
                original_text_ref = point.get("original_text", "")  # è¯„ä¼°é˜¶æ®µçš„å‚è€ƒåŸæ–‡
                modification_reason = point.get("modification_reason", "")
                modification_type = point.get("modification_type", "")
                is_full_chapter = point.get("is_full_chapter", False)
                
                # æ£€æŸ¥original_texté•¿åº¦
                original_length = len(original_text_ref)
                
                logger.info(f"ğŸ”„ ä¿®æ”¹ç‚¹ {idx}/{len(modification_points)}: [{location}] - {modification_type}")
                logger.info(f"   åŸæ–‡é•¿åº¦: {original_length} å­—ç¬¦, å®Œæ•´ç« èŠ‚: {is_full_chapter}")
                
                # æ ¹æ®is_full_chapterå’Œoriginal_lengthå†³å®šç”Ÿæˆç­–ç•¥
                if is_full_chapter or original_length > 1000:
                    task_type = "ç« èŠ‚é‡å†™"
                    length_requirement = f"å¿…é¡»ç”Ÿæˆä¸åŸæ–‡ç­‰é•¿çš„å®Œæ•´ç« èŠ‚ï¼ˆ{original_length}å­—ç¬¦å·¦å³ï¼Œå…è®¸Â±10%ï¼‰"
                    structure_requirement = "ä¿æŒåŸç« èŠ‚çš„æ‰€æœ‰å­ç« èŠ‚ç»“æ„ï¼ˆ##ã€###ç­‰ï¼‰"
                else:
                    task_type = "æ®µè½ä¿®æ”¹"
                    length_requirement = f"ä¿æŒæ®µè½é•¿åº¦ï¼ˆçº¦{original_length}å­—ç¬¦ï¼‰"
                    structure_requirement = "ä¿æŒæ®µè½æ ¼å¼"
                
                # æ„å»ºReactAgentçš„ä»»åŠ¡
                react_task = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ä¿®æ”¹åŠ©æ‰‹ã€‚è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å®Œæˆä»»åŠ¡ï¼š

ã€ä¿®æ”¹è¦æ±‚ã€‘
{modification_request}

ã€ä¿®æ”¹ä½ç½®ã€‘{location}

ã€ä¿®æ”¹åŸå› ä¸æ·±åº¦åˆ†æã€‘
{modification_reason}

ã€ä¿®æ”¹ç±»å‹ã€‘{modification_type}

ã€åŸæ–‡å†…å®¹ã€‘ï¼ˆ{original_length}å­—ç¬¦ï¼‰
```
{original_text_ref}
```

ã€å·¥ä½œæµç¨‹ã€‘

**é˜¶æ®µ1ï¼šç†è§£ä¿®æ”¹çš„æ·±å±‚ç›®çš„**
- ä»”ç»†é˜…è¯»ã€ä¿®æ”¹åŸå› ä¸æ·±åº¦åˆ†æã€‘ï¼Œç†è§£ä¸ºä»€ä¹ˆè¦ä¿®æ”¹è¿™éƒ¨åˆ†å†…å®¹
- è¿™ä¸æ˜¯ç®€å•çš„æ–‡å­—æ›¿æ¢ï¼Œè€Œæ˜¯è¦æ ¹æ®åˆ†æä¸­æåˆ°çš„ç›®çš„å’Œæ„ä¹‰è¿›è¡Œä¿®æ”¹
- æ€è€ƒï¼šä¿®æ”¹ååº”è¯¥è¾¾åˆ°ä»€ä¹ˆæ•ˆæœï¼Ÿå¯¹è¯»è€…æœ‰ä»€ä¹ˆå¸®åŠ©ï¼Ÿ

**é˜¶æ®µ2ï¼šæœç´¢èµ„æ–™ï¼ˆå¦‚æœéœ€è¦ï¼‰**
- å¦‚æœä¿®æ”¹è¦æ±‚æ˜¯ç®€å•çš„æœ¯è¯­æ›¿æ¢ï¼ˆå¦‚"å°†MemOSæ”¹ä¸ºmem0"ï¼‰ï¼Œ**ä¸éœ€è¦æœç´¢**ï¼Œç›´æ¥è¿›å…¥é˜¶æ®µ3
- å¦‚æœä¿®æ”¹è¦æ±‚æ¶‰åŠå†…å®¹è¡¥å……ã€æ‰©å±•ã€ä¼˜åŒ–ï¼Œ**å¯ä»¥å¤šæ¬¡æœç´¢**ç›¸å…³èµ„æ–™
- æ ¹æ®ã€ä¿®æ”¹åŸå› ä¸æ·±åº¦åˆ†æã€‘ä¸­æåˆ°çš„éœ€æ±‚ï¼Œæœ‰é’ˆå¯¹æ€§åœ°æœç´¢
- æœç´¢ç­–ç•¥ï¼š
  * ç¬¬1æ¬¡æœç´¢ï¼šæ ¸å¿ƒæ¦‚å¿µå’Œå®šä¹‰
  * ç¬¬2æ¬¡æœç´¢ï¼ˆå¦‚éœ€è¦ï¼‰ï¼šç›¸å…³æŠ€æœ¯ç»†èŠ‚
  * ç¬¬3æ¬¡æœç´¢ï¼ˆå¦‚éœ€è¦ï¼‰ï¼šåº”ç”¨æ¡ˆä¾‹æˆ–æœ€æ–°ç ”ç©¶
- æœç´¢å¤Ÿäº†å°±åœæ­¢ï¼Œä¸è¦è¿‡åº¦æœç´¢

**é˜¶æ®µ3ï¼šç”Ÿæˆä¿®æ”¹åçš„å†…å®¹ï¼ˆåªèƒ½1æ¬¡ï¼‰**
- åŸºäºåŸæ–‡ã€ã€ä¿®æ”¹åŸå› ä¸æ·±åº¦åˆ†æã€‘å’Œæœç´¢åˆ°çš„èµ„æ–™ï¼Œ**ä¸€æ¬¡æ€§**ç”Ÿæˆå®Œæ•´çš„ä¿®æ”¹åå†…å®¹
- **ç¦æ­¢**ç”Ÿæˆåå†ç»§ç»­è¿­ä»£æˆ–ç»§ç»­ç”Ÿæˆ
- ç”Ÿæˆå®Œç«‹å³finish

ã€ç”Ÿæˆè¦æ±‚ã€‘

1. **å®Œæ•´æ€§**ï¼š
   - å¿…é¡»è¦†ç›–åŸæ–‡çš„æ‰€æœ‰å†…å®¹ï¼ˆ{original_length}å­—ç¬¦ï¼‰
   - {structure_requirement}
   - ä¸è¦æˆªæ–­ï¼Œä¸è¦åªç”Ÿæˆå¼€å¤´éƒ¨åˆ†
   - ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å†…å®¹

2. **ä¿®æ”¹å‡†ç¡®æ€§**ï¼š
   - æ ¹æ®ã€ä¿®æ”¹åŸå› ä¸æ·±åº¦åˆ†æã€‘ä¸­çš„æ·±å±‚ç›®çš„è¿›è¡Œä¿®æ”¹ï¼Œè€Œä¸æ˜¯ç®€å•çš„æ–‡å­—æ›¿æ¢
   - ä¸¥æ ¼æŒ‰ç…§ä¿®æ”¹è¦æ±‚æ‰§è¡Œï¼ˆå¦‚"MemOS"â†’"mem0"ï¼‰
   - ä¿æŒåŸæ–‡çš„ç»“æ„ã€é€»è¾‘ã€å­¦æœ¯é£æ ¼
   - å¦‚æœã€ä¿®æ”¹åŸå› ä¸æ·±åº¦åˆ†æã€‘ä¸­æåˆ°äº†ç‰¹å®šçš„ä¿®æ”¹æ„ä¹‰æˆ–é¢„æœŸæ•ˆæœï¼Œè¦åœ¨ç”Ÿæˆå†…å®¹ä¸­ä½“ç°å‡ºæ¥
   - åªä¿®æ”¹éœ€è¦æ”¹çš„éƒ¨åˆ†ï¼Œä¸è¦å¤§å¹…æ”¹å†™

3. **æ ¼å¼è§„èŒƒ**ï¼š
   - ä¿ç•™æ‰€æœ‰Markdownæ ¼å¼
   - ä¸è¦æ·»åŠ ```ä»£ç å—æ ‡è®°
   - ç›´æ¥è¾“å‡ºçº¯æ–‡æœ¬

4. **é•¿åº¦æ§åˆ¶**ï¼š
   - ç›®æ ‡é•¿åº¦ï¼š{length_requirement}
   - å¦‚æœæ˜æ˜¾åçŸ­ï¼Œè¯´æ˜å†…å®¹ä¸å®Œæ•´

ã€é‡è¦ã€‘
- ç”Ÿæˆæ—¶ä¸€æ¬¡æ€§è¾“å‡ºå®Œæ•´å†…å®¹ï¼Œä¸è¦åˆ†æ®µç”Ÿæˆ
- ç”Ÿæˆåç«‹å³è¿”å›finishï¼Œä¸è¦ç»§ç»­è¿­ä»£
"""
                
                # ä½¿ç”¨ReactAgentç”Ÿæˆå†…å®¹
                result = await agent.run(react_task)
                
                content = result.get("content", "").strip()
                thinking = result.get("thinking_process", [])
                search_history = result.get("search_history", [])
                
                # ç›´æ¥ä½¿ç”¨ReactAgentè¿”å›çš„å†…å®¹ä½œä¸ºä¿®æ”¹åçš„æ–‡æœ¬
                # ä½¿ç”¨è¯„ä¼°é˜¶æ®µçš„original_text_refæ¥å®šä½
                final_modified = content
                final_original = original_text_ref
                
                logger.info(f"âœ… ä¿®æ”¹ç‚¹ {idx} å®Œæˆ")
                logger.info(f"   ç”Ÿæˆå†…å®¹é•¿åº¦: {len(final_modified)} å­—ç¬¦")
            
                logger.info(f"âœ… ä¿®æ”¹ç‚¹ {idx} å¤„ç†å®Œæˆ: å°†ç”¨äºå®šä½çš„åŸæ–‡é•¿åº¦ {len(final_original)} â†’ ä¿®æ”¹å {len(final_modified)} å­—ç¬¦")
                
                return {
                    "modification": {
                        "location": location,
                        "original_text": final_original,  # ä½¿ç”¨è¯„ä¼°é˜¶æ®µçš„original_text_ref
                        "modified_text": final_modified,   # ä½¿ç”¨ReactAgentç”Ÿæˆçš„ä¿®æ”¹åå†…å®¹
                        "reason": modification_reason,
                        "modification_type": modification_type
                    },
                    "thinking": {
                        "modification_point": idx,
                        "location": location,
                        "thinking_steps": thinking,
                        "generated_length": len(final_modified),
                        "used_react_original": False
                    },
                    "search_history": search_history
                }
            except Exception as e:
                logger.error(f"âŒ ä¿®æ”¹ç‚¹ {idx} [{location}] å¤„ç†å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                # è¿”å›ä¸€ä¸ªé»˜è®¤çš„ç»“æœï¼Œä¿æŒåŸæ–‡ä¸å˜
                return {
                    "modification": {
                        "location": location,
                        "original_text": original_text_ref,
                        "modified_text": original_text_ref,  # ä¿æŒåŸæ ·
                        "reason": f"å¤„ç†å¤±è´¥: {str(e)}",
                        "modification_type": modification_type
                    },
                    "thinking": {
                        "modification_point": idx,
                        "location": location,
                        "thinking_steps": [],
                        "generated_length": len(original_text_ref),
                        "used_react_original": False
                    },
                    "search_history": []
                }
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰ä¿®æ”¹ç‚¹
        tasks = [
            process_single_point(idx, point) 
            for idx, point in enumerate(modification_points, 1)
        ]
        results = await asyncio.gather(*tasks)
        
        # æ•´ç†ç»“æœ
        modifications_list = [r["modification"] for r in results]
        all_thinking_process = [r["thinking"] for r in results]
        all_search_history = []
        for r in results:
            all_search_history.extend(r["search_history"])
        
        # åº”ç”¨æ‰€æœ‰ä¿®æ”¹åˆ°åŸæ–‡
        logger.info(f"\nğŸ“ åº”ç”¨ {len(modifications_list)} å¤„ä¿®æ”¹åˆ°åŸæ–‡...")
        modified_content = self._apply_diff_modifications(original_content, modifications_list)
        
        logger.info(f"âœ… æ‰€æœ‰ä¿®æ”¹å®Œæˆ")
        logger.info(f"   åŸæ–‡: {len(original_content)} â†’ ä¿®æ”¹å: {len(modified_content)} å­—ç¬¦")
        logger.info(f"   å˜åŒ–: {len(modified_content) - len(original_content):+d} å­—ç¬¦")
        
        return {
            "content": modified_content,
            "thinking_process": all_thinking_process,
            "search_history": all_search_history
        }
    
    async def _generate_modifications_with_rag(self,
                                              modification_request: str,
                                              original_content: str,
                                              modification_points: List[Dict],
                                              reference_materials: str) -> List[Dict]:
        """
        åŸºäºRAGæœç´¢èµ„æ–™å’Œè¯„ä¼°ç»“æœï¼Œç”Ÿæˆå…·ä½“çš„ä¿®æ”¹å»ºè®®ï¼ˆJSON diffæ ¼å¼ï¼‰
        
        Args:
            modification_request: ä¿®æ”¹è¦æ±‚
            original_content: åŸå§‹æ–‡æ¡£å†…å®¹
            modification_points: è¯„ä¼°å‡ºçš„ä¿®æ”¹ç‚¹åˆ—è¡¨
            reference_materials: RAGæœç´¢åˆ°çš„å‚è€ƒèµ„æ–™
            
        Returns:
            modificationsåˆ—è¡¨: [{"location": "...", "original_text": "...", "modified_text": "...", "reason": "..."}]
        """
        # æ„å»ºä¿®æ”¹ç‚¹æ‘˜è¦
        points_summary = "\n".join([
            f"{idx}. ä½ç½®ï¼š{point.get('location', 'æœªçŸ¥')}\n"
            f"   åŸæ–‡ç‰‡æ®µï¼š{point.get('original_text', '')[:200]}...\n"
            f"   ä¿®æ”¹åŸå› ï¼š{point.get('modification_reason', '')}\n"
            f"   ä¿®æ”¹ç±»å‹ï¼š{point.get('modification_type', '')}"
            for idx, point in enumerate(modification_points, 1)
        ])
        
        prompt = f"""ä½ éœ€è¦æ ¹æ®è¯„ä¼°ç»“æœå’Œå‚è€ƒèµ„æ–™ï¼Œä¸ºæ–‡æ¡£ç”Ÿæˆå…·ä½“çš„ä¿®æ”¹å»ºè®®ã€‚

ä¿®æ”¹è¦æ±‚ï¼š
{modification_request}

è¯„ä¼°å‡ºçš„ä¿®æ”¹ç‚¹ï¼š
{points_summary}

å‚è€ƒèµ„æ–™ï¼š
{reference_materials if reference_materials else "æ— "}

åŸå§‹æ–‡æ¡£ï¼ˆéƒ¨åˆ†ï¼‰ï¼š
{original_content[:2000]}...

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºæ¯ä¸ªä¿®æ”¹ç‚¹ç”Ÿæˆå…·ä½“çš„ä¿®æ”¹å†…å®¹ã€‚ä½¿ç”¨JSONæ ¼å¼è¾“å‡ºï¼š
```json
{{
  "modifications": [
    {{
      "location": "ç« èŠ‚åç§°æˆ–ä½ç½®æè¿°",
      "original_text": "éœ€è¦æ›¿æ¢çš„åŸå§‹æ–‡æœ¬ï¼ˆä»æ–‡æ¡£ä¸­ç²¾ç¡®å¤åˆ¶ï¼‰",
      "modified_text": "ä¿®æ”¹åçš„æ–‡æœ¬ï¼ˆå¯ä»¥å‚è€ƒRAGèµ„æ–™å®Œå–„å†…å®¹ï¼‰",
      "reason": "ä¿®æ”¹åŸå› "
    }}
  ]
}}
```

**é‡è¦è¦æ±‚**ï¼š
1. original_textå¿…é¡»ä»æ–‡æ¡£ä¸­ç²¾ç¡®å¤åˆ¶ï¼Œç”¨äºå®šä½
2. modified_textåº”è¯¥å‚è€ƒRAGæœç´¢åˆ°çš„èµ„æ–™ï¼ˆå¦‚æœæœ‰ï¼‰æ¥å®Œå–„
3. ä¿æŒåŸæ–‡çš„æ ¼å¼å’Œé£æ ¼
4. åªä¿®æ”¹å¿…è¦çš„éƒ¨åˆ†ï¼Œä¸è¦å¤§å¹…æ”¹å†™

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘ï¼Œæ“…é•¿åŸºäºå‚è€ƒèµ„æ–™ç”Ÿæˆç²¾ç¡®çš„ä¿®æ”¹å»ºè®®ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=3000
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # æå–JSON
            if "```json" in raw_response:
                json_str = raw_response.split("```json")[1].split("```")[0].strip()
            elif "```" in raw_response:
                json_str = raw_response.split("```")[1].split("```")[0].strip()
            else:
                json_str = raw_response
            
            modifications_data = json.loads(json_str)
            modifications_list = modifications_data.get("modifications", [])
            
            logger.info(f"ğŸ“ ç”Ÿæˆäº† {len(modifications_list)} ä¸ªä¿®æ”¹å»ºè®®")
            for idx, mod in enumerate(modifications_list, 1):
                logger.info(f"  {idx}. [{mod.get('location', 'æœªçŸ¥')}] {mod.get('reason', '')}")
            
            return modifications_list
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆä¿®æ”¹å»ºè®®å¤±è´¥: {str(e)}")
            # é™çº§æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨è¯„ä¼°ç‚¹ä½œä¸ºä¿®æ”¹å»ºè®®
            return [{
                "location": point.get("location", "æœªçŸ¥"),
                "original_text": point.get("original_text", ""),
                "modified_text": point.get("original_text", ""),  # ä¿æŒåŸæ ·
                "reason": f"ç”Ÿæˆå¤±è´¥ï¼Œä¿æŒåŸæ ·: {str(e)}"
            } for point in modification_points]
    
    async def read_file_content(self, minio_url: str) -> str:
        """
        è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆä»MinIOï¼‰
        
        Args:
            minio_url: MinIOæ–‡ä»¶URLï¼ˆæ¥è‡ªRAG metadataï¼‰
            
        Returns:
            æ–‡ä»¶å†…å®¹
        """
        try:
            # ä»MinIOè¯»å–æ–‡ä»¶
            content = await self.kb_manager.read_file_from_minio(minio_url)
            
            if content:
                logger.info(f"æˆåŠŸä»MinIOè¯»å–æ–‡ä»¶, é•¿åº¦: {len(content)} å­—ç¬¦")
                return content
            else:
                logger.error(f"ä»MinIOè¯»å–æ–‡ä»¶å¤±è´¥: {minio_url}")
                return ""
                
        except Exception as e:
            logger.error(f"è¯»å–MinIOæ–‡ä»¶å¼‚å¸¸ {minio_url}: {str(e)}")
            return ""
    
    def _apply_diff_modifications(self, original_content: str, modifications: list) -> str:
        """
        å°†JSONæ ¼å¼çš„ä¿®æ”¹åº”ç”¨åˆ°åŸæ–‡æ¡£ï¼ˆæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…ç‰ˆæœ¬ï¼‰
        
        Args:
            original_content: åŸå§‹æ–‡æ¡£å†…å®¹
            modifications: [{"location": "...", "original_text": "...", "modified_text": "...", "reason": "..."}]
            
        Returns:
            ä¿®æ”¹åçš„æ–‡æ¡£å†…å®¹
        """
        result = original_content
        applied_count = 0
        failed_mods = []
        skipped_duplicates = []
        
        # æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºæ¯”è¾ƒ
        def normalize_text(text):
            """æ ‡å‡†åŒ–æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€ç»Ÿä¸€æ¢è¡Œã€å»é™¤çœç•¥å·"""
            # å»é™¤çœç•¥å·
            text = text.replace('...', ' ')
            text = text.replace('â€¦', ' ')
            # ç»Ÿä¸€ç©ºç™½å­—ç¬¦
            text = ' '.join(text.split())
            return text.strip()
        
        def fuzzy_find_in_content(search_text, content, threshold=0.8):
            """
            åœ¨å†…å®¹ä¸­æ¨¡ç³ŠæŸ¥æ‰¾æ–‡æœ¬
            
            Args:
                search_text: è¦æŸ¥æ‰¾çš„æ–‡æœ¬ï¼ˆå¯èƒ½ä¸ç²¾ç¡®ï¼‰
                content: å†…å®¹
                threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
                
            Returns:
                (æ‰¾åˆ°çš„æ–‡æœ¬, èµ·å§‹ä½ç½®) æˆ– (None, -1)
            """
            search_normalized = normalize_text(search_text)
            search_words = search_normalized.split()
            
            # å¦‚æœsearch_textå¤ªçŸ­ï¼Œç›´æ¥ç²¾ç¡®æŸ¥æ‰¾
            if len(search_normalized) < 20:
                if search_text in content:
                    return search_text, content.find(search_text)
                return None, -1
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = content.split('\n\n')
            
            for p_idx, paragraph in enumerate(paragraphs):
                para_normalized = normalize_text(paragraph)
                para_words = para_normalized.split()
                
                # æ£€æŸ¥å…³é”®è¯åŒ¹é…åº¦
                if len(search_words) > 0:
                    # è®¡ç®—æœ‰å¤šå°‘ä¸ªå…³é”®è¯å‡ºç°åœ¨æ®µè½ä¸­
                    matched_words = sum(1 for word in search_words if word in para_words)
                    similarity = matched_words / len(search_words)
                    
                    if similarity >= threshold:
                        # æ‰¾åˆ°åŒ¹é…çš„æ®µè½
                        return paragraph, content.find(paragraph)
            
            # å°è¯•æŒ‰å¥å­æŸ¥æ‰¾ï¼ˆå¤„ç†è·¨æ®µè½çš„æƒ…å†µï¼‰
            sentences = content.replace('\n\n', '\n').split('\n')
            for sent in sentences:
                sent_normalized = normalize_text(sent)
                sent_words = sent_normalized.split()
                
                if len(search_words) > 0:
                    matched_words = sum(1 for word in search_words if word in sent_words)
                    similarity = matched_words / len(search_words)
                    
                    if similarity >= threshold:
                        return sent, content.find(sent)
            
            return None, -1
        
        # å»é‡ + æ™ºèƒ½æ‰©å±•åŸæ–‡æå–
        seen_originals = set()
        deduplicated_mods = []
        for mod in modifications:
            original_text = mod.get("original_text", "").strip()
            modified_text = mod.get("modified_text", "").strip()
            location = mod.get("location", "æœªçŸ¥")
            
            # æ³¨æ„ï¼šoriginal_textçš„æ‰©å±•å·²ç»åœ¨è¯„ä¼°é˜¶æ®µå®Œæˆ
            # è¿™é‡Œä¸å†éœ€è¦äºŒæ¬¡æ‰©å±•
            
            original_normalized = normalize_text(original_text)
            
            if original_text and original_normalized not in seen_originals:
                seen_originals.add(original_normalized)
                deduplicated_mods.append(mod)
            elif original_text:
                logger.info(f"âš ï¸ è·³è¿‡é‡å¤çš„ä¿®æ”¹ç‚¹: {location}")
                logger.info(f"   å†…å®¹: {original_text[:60]}...")
                skipped_duplicates.append(location)
        
        if skipped_duplicates:
            logger.info(f"ğŸ”„ å»é‡: è·³è¿‡äº† {len(skipped_duplicates)} ä¸ªé‡å¤çš„ä¿®æ”¹ç‚¹")
        
        # æŒ‰é¡ºåºåº”ç”¨æ¯ä¸ªä¿®æ”¹
        for idx, mod in enumerate(deduplicated_mods, 1):
            original_text = mod.get("original_text", "").strip()
            modified_text = mod.get("modified_text", "").strip()
            location = mod.get("location", "æœªæŒ‡å®šä½ç½®")
            reason = mod.get("reason", "")
            
            if not original_text:
                logger.warning(f"âš ï¸ ä¿®æ”¹ #{idx} [{location}]: ç¼ºå°‘original_text")
                failed_mods.append(f"{location} (ç¼ºå°‘åŸæ–‡)")
                continue
            
            # æ ‡å‡†åŒ–æ¯”è¾ƒ
            original_normalized = normalize_text(original_text)
            modified_normalized = normalize_text(modified_text)
            
            # æ£€æµ‹æ˜¯å¦çœŸçš„æœ‰ä¿®æ”¹
            if original_normalized == modified_normalized:
                logger.info(f"â­ï¸  ä¿®æ”¹ #{idx} [{location}]: å†…å®¹å®è´¨æœªå˜åŒ–ï¼Œè·³è¿‡")
                logger.info(f"   åŸæ–‡: {original_text[:60]}...")
                continue
            
            # æ–¹æ³•1: ç²¾ç¡®åŒ¹é…
            if original_text in result:
                # ç›´æ¥æ›¿æ¢ï¼Œä¸éœ€è¦é¢å¤–çš„é‡å¤æ£€æŸ¥
                # å› ä¸ºæˆ‘ä»¬æ‰¾åˆ°äº†åŸæ–‡ï¼Œå°±åº”è¯¥æ›¿æ¢å®ƒï¼Œä¸ç®¡æ›¿æ¢åçš„å†…å®¹æ˜¯ä»€ä¹ˆ
                result = result.replace(original_text, modified_text, 1)
                applied_count += 1
                logger.info(f"âœ… ä¿®æ”¹ #{idx} [{location}] (ç²¾ç¡®åŒ¹é…)")
                logger.info(f"   {len(original_text)} å­—ç¬¦ â†’ {len(modified_text)} å­—ç¬¦")
                if reason:
                    logger.info(f"   åŸå› : {reason}")
            else:
                # æ–¹æ³•2: æ¨¡ç³ŠåŒ¹é…
                logger.info(f"ğŸ” å°è¯•æ¨¡ç³ŠåŒ¹é…ä¿®æ”¹ç‚¹ #{idx} [{location}]...")
                found_text, pos = fuzzy_find_in_content(original_text, result, threshold=0.7)
                
                if found_text and pos != -1:
                    # æ‰¾åˆ°äº†åŒ¹é…çš„æ–‡æœ¬
                    logger.info(f"âœ… ä¿®æ”¹ #{idx} [{location}] (æ¨¡ç³ŠåŒ¹é…ï¼Œç›¸ä¼¼åº¦>=70%)")
                    logger.info(f"   æ‰¾åˆ°çš„æ–‡æœ¬: {found_text[:80]}...")
                    
                    # æ›¿æ¢æ‰¾åˆ°çš„æ–‡æœ¬
                    result = result.replace(found_text, modified_text, 1)
                    applied_count += 1
                    if reason:
                        logger.info(f"   åŸå› : {reason}")
                else:
                    # æ–¹æ³•3: é™ä½é˜ˆå€¼å†è¯•ä¸€æ¬¡
                    found_text, pos = fuzzy_find_in_content(original_text, result, threshold=0.5)
                    
                    if found_text and pos != -1:
                        logger.info(f"âœ… ä¿®æ”¹ #{idx} [{location}] (ä½ç›¸ä¼¼åº¦åŒ¹é…ï¼Œç›¸ä¼¼åº¦>=50%)")
                        logger.info(f"   æ‰¾åˆ°çš„æ–‡æœ¬: {found_text[:80]}...")
                        logger.warning(f"   âš ï¸ æ³¨æ„ï¼šæ­¤åŒ¹é…ç›¸ä¼¼åº¦è¾ƒä½ï¼Œè¯·æ£€æŸ¥ç»“æœ")
                        
                        # æ›¿æ¢æ‰¾åˆ°çš„æ–‡æœ¬
                        result = result.replace(found_text, modified_text, 1)
                        applied_count += 1
                    else:
                        # å®Œå…¨æ— æ³•å®šä½
                        logger.warning(f"âŒ ä¿®æ”¹ #{idx} [{location}]: æ— æ³•å®šä½ï¼ˆå³ä½¿ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰")
                        logger.warning(f"   æŸ¥æ‰¾æ–‡æœ¬: {original_text[:100]}...")
                        logger.warning(f"   æç¤ºï¼šAIæå–çš„åŸæ–‡å¯èƒ½ä¸å‡†ç¡®ï¼ŒåŒ…å«çœç•¥å·æˆ–æ ¼å¼é—®é¢˜")
                        failed_mods.append(location)
        
        logger.info(f"\nğŸ“Š ä¿®æ”¹ç»Ÿè®¡:")
        logger.info(f"   æ€»ä¿®æ”¹ç‚¹: {len(modifications)}")
        logger.info(f"   å»é‡å: {len(deduplicated_mods)}")
        logger.info(f"   æˆåŠŸåº”ç”¨: {applied_count}")
        logger.info(f"   å¤±è´¥: {len(failed_mods)}")
        
        if failed_mods:
            logger.warning(f"âš ï¸ æœªåº”ç”¨çš„ä¿®æ”¹: {', '.join(failed_mods)}")
        
        return result
    
    def _deduplicate_hierarchical_chapters(self, modification_points: List[Dict], document: str) -> List[Dict]:
        """
        å»é™¤é‡å¤çš„çˆ¶å­ç« èŠ‚
        
        ä¾‹å¦‚ï¼šå¦‚æœåŒæ—¶æœ‰ "# 3 ç« èŠ‚" å’Œ "## 3.1 å°èŠ‚"ï¼Œåªä¿ç•™çˆ¶ç« èŠ‚
        
        Args:
            modification_points: ä¿®æ”¹ç‚¹åˆ—è¡¨
            document: å®Œæ•´æ–‡æ¡£
            
        Returns:
            å»é‡åçš„ä¿®æ”¹ç‚¹åˆ—è¡¨
        """
        if not modification_points:
            return modification_points
        
        # æå–æ¯ä¸ªä¿®æ”¹ç‚¹çš„æ ‡é¢˜çº§åˆ«å’Œç« èŠ‚ç¼–å·
        points_with_meta = []
        for point in modification_points:
            original_text = point.get('original_text', '').strip()
            if not original_text.startswith('#'):
                # ä¸æ˜¯æ ‡é¢˜ï¼Œä¿ç•™
                points_with_meta.append({
                    'point': point,
                    'is_title': False,
                    'level': 999,  # éæ ‡é¢˜ï¼Œçº§åˆ«æœ€ä½
                    'chapter_num': None
                })
                continue
            
            # æå–æ ‡é¢˜çº§åˆ«ï¼ˆ#çš„æ•°é‡ï¼‰
            title_line = original_text.split('\n')[0]
            level = 0
            for char in title_line:
                if char == '#':
                    level += 1
                else:
                    break
            
            # æå–ç« èŠ‚ç¼–å·ï¼ˆå¦‚ "3", "3.1", "4.2"ï¼‰
            # å‡è®¾æ ¼å¼ä¸º "# 3 ç« èŠ‚å" æˆ– "## 3.1 å°èŠ‚å"
            import re
            chapter_match = re.search(r'#\s+(\d+(?:\.\d+)*)', title_line)
            chapter_num = chapter_match.group(1) if chapter_match else None
            
            points_with_meta.append({
                'point': point,
                'is_title': True,
                'level': level,
                'chapter_num': chapter_num,
                'title': title_line
            })
        
        # æ£€æµ‹å¹¶ç§»é™¤å­ç« èŠ‚ï¼ˆå¦‚æœçˆ¶ç« èŠ‚å­˜åœ¨ï¼‰
        to_remove = set()
        for i, meta_i in enumerate(points_with_meta):
            if not meta_i['is_title'] or meta_i['chapter_num'] is None:
                continue
            
            for j, meta_j in enumerate(points_with_meta):
                if i == j or not meta_j['is_title'] or meta_j['chapter_num'] is None:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºçˆ¶å­å…³ç³»
                # ä¾‹å¦‚ï¼šchapter_i="3", chapter_j="3.1" â†’ jæ˜¯içš„å­ç« èŠ‚
                if (meta_j['level'] > meta_i['level'] and 
                    meta_j['chapter_num'].startswith(meta_i['chapter_num'] + '.')):
                    # meta_jæ˜¯meta_içš„å­ç« èŠ‚ï¼Œæ ‡è®°åˆ é™¤
                    to_remove.add(j)
                    logger.warning(f"ğŸ”„ æ£€æµ‹åˆ°çˆ¶å­ç« èŠ‚é‡å¤:")
                    logger.warning(f"   çˆ¶ç« èŠ‚: {meta_i['title']}")
                    logger.warning(f"   å­ç« èŠ‚: {meta_j['title']} â† å°†è¢«ç§»é™¤ï¼ˆå·²åŒ…å«åœ¨çˆ¶ç« èŠ‚ä¸­ï¼‰")
        
        # ç§»é™¤é‡å¤çš„å­ç« èŠ‚
        deduplicated = [meta['point'] for i, meta in enumerate(points_with_meta) if i not in to_remove]
        
        if to_remove:
            logger.info(f"âœ… ç§»é™¤äº† {len(to_remove)} ä¸ªé‡å¤çš„å­ç« èŠ‚")
        
        return deduplicated
    
    def _expand_original_text(self, document: str, partial_text: str) -> str:
        """
        æ™ºèƒ½æ‰©å±•åŸæ–‡æå–èŒƒå›´
        
        æ ¸å¿ƒåŠŸèƒ½ï¼šå°†Markdownæ ‡é¢˜æ‰©å±•ä¸ºå®Œæ•´ç« èŠ‚å†…å®¹
        
        Args:
            document: å®Œæ•´æ–‡æ¡£
            partial_text: éƒ¨åˆ†æå–çš„åŸæ–‡ï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜ï¼‰
            
        Returns:
            æ‰©å±•åçš„å®Œæ•´ç« èŠ‚å†…å®¹
        """
        if not partial_text:
            return partial_text
        
        partial_text_stripped = partial_text.strip()
        
        # æ‰¾åˆ°partial_textåœ¨æ–‡æ¡£ä¸­çš„ä½ç½®ï¼ˆå¿½ç•¥å‰åç©ºç™½ï¼‰
        start_pos = document.find(partial_text_stripped)
        if start_pos == -1:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼ï¼‰
            import re
            normalized_partial = re.sub(r'\s+', ' ', partial_text_stripped)
            normalized_doc = re.sub(r'\s+', ' ', document)
            start_pos_normalized = normalized_doc.find(normalized_partial)
            if start_pos_normalized != -1:
                # åœ¨åŸæ–‡æ¡£ä¸­æ‰¾åˆ°å¯¹åº”ä½ç½®
                # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥æ‰¾
                pass
            else:
                logger.warning(f"âš ï¸ æ— æ³•åœ¨æ–‡æ¡£ä¸­æ‰¾åˆ°åŸæ–‡: {partial_text_stripped[:100]}")
                return partial_text
        
        # æ£€æµ‹æ˜¯å¦ä¸ºMarkdownæ ‡é¢˜ï¼ˆä»¥#å¼€å¤´ï¼‰
        if partial_text_stripped.startswith('#'):
            logger.info(f"ğŸ” æ£€æµ‹åˆ°æ ‡é¢˜ï¼Œå¼€å§‹æ‰©å±•åˆ°å®Œæ•´ç« èŠ‚...")
            
            # æå–æ ‡é¢˜çº§åˆ«ï¼ˆ#çš„æ•°é‡ï¼‰
            title_match = partial_text_stripped.split('\n')[0]  # åªçœ‹ç¬¬ä¸€è¡Œ
            title_level = 0
            for char in title_match:
                if char == '#':
                    title_level += 1
                else:
                    break
            
            logger.info(f"   æ ‡é¢˜çº§åˆ«: {'#' * title_level} (level {title_level})")
            
            # ä»start_poså¼€å§‹æŸ¥æ‰¾è¿™ä¸ªç« èŠ‚çš„ç»“æŸä½ç½®
            # ç»“æŸä½ç½® = ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§çš„æ ‡é¢˜
            chapter_start = start_pos
            chapter_end = len(document)  # é»˜è®¤åˆ°æ–‡æ¡£æœ«å°¾
            
            # åœ¨start_posä¹‹åæŸ¥æ‰¾ä¸‹ä¸€ä¸ªæ ‡é¢˜
            lines_after = document[start_pos + len(partial_text_stripped):].split('\n')
            chars_accumulated = start_pos + len(partial_text_stripped)
            
            for line in lines_after:
                chars_accumulated += len(line) + 1  # +1 for \n
                
                line_stripped = line.strip()
                if line_stripped.startswith('#'):
                    # æ‰¾åˆ°ä¸€ä¸ªæ ‡é¢˜ï¼Œæ£€æŸ¥çº§åˆ«
                    current_level = 0
                    for char in line_stripped:
                        if char == '#':
                            current_level += 1
                        else:
                            break
                    
                    # å¦‚æœæ˜¯åŒçº§æˆ–æ›´é«˜çº§çš„æ ‡é¢˜ï¼Œè¿™é‡Œå°±æ˜¯ç« èŠ‚ç»“æŸ
                    if current_level <= title_level:
                        chapter_end = chars_accumulated - len(line) - 1  # å›é€€åˆ°è¿™ä¸€è¡Œä¹‹å‰
                        logger.info(f"   æ‰¾åˆ°ä¸‹ä¸€ä¸ªåŒçº§æ ‡é¢˜: {'#' * current_level} {line_stripped[:50]}")
                        break
            
            # æå–å®Œæ•´ç« èŠ‚
            full_chapter = document[chapter_start:chapter_end].strip()
            
            logger.info(f"   âœ… æ‰©å±•æˆåŠŸ: {len(partial_text_stripped)} â†’ {len(full_chapter)} å­—ç¬¦")
            logger.info(f"   ç« èŠ‚é¢„è§ˆ: {full_chapter[:100]}...")
            
            return full_chapter
        
        # å¦‚æœä¸æ˜¯æ ‡é¢˜ï¼Œå°è¯•æ‰©å±•åˆ°æ®µè½ç»“æŸ
        else:
            end_pos = start_pos + len(partial_text_stripped)
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªåŒæ¢è¡Œï¼ˆæ®µè½ç»“æŸï¼‰
            next_para = document.find('\n\n', end_pos)
            if next_para != -1 and next_para - start_pos < 2000:
                paragraph = document[start_pos:next_para].strip()
                logger.info(f"   æ‰©å±•æ®µè½: {len(partial_text_stripped)} â†’ {len(paragraph)} å­—ç¬¦")
                return paragraph
        
        return partial_text
    
    def _generate_diff_summary(self, original: str, modified: str) -> str:
        """ç”Ÿæˆç®€å•çš„diffæ‘˜è¦"""
        orig_lines = original.split('\n')
        mod_lines = modified.split('\n')
        
        added = len(mod_lines) - len(orig_lines)
        
        # ç®€å•ç»Ÿè®¡å˜åŒ–
        return f"è¡Œæ•°å˜åŒ–: {added:+d}ï¼ŒåŸ{len(orig_lines)}è¡Œ â†’ æ–°{len(mod_lines)}è¡Œ"

